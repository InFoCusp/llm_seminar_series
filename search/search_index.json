{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Large Language Models Seminar Series","text":"<p>A multi-part seminar series on Large Language Models (LLMs). \ud83c\udf10 Website  | \ud83e\udde0 LLM Full Mind Map </p> <p></p>"},{"location":"#emergence-fundamentals-and-landscape-of-llms","title":"\u2728 Emergence, Fundamentals and Landscape of LLMs","text":"<p>Covers important building blocks of what we call an LLM today, where they came from, etc. and then we'll dive into the deep universe that has sprung to life around these LLMs.</p> <p></p>"},{"location":"#universe-of-pretrained-llms-and-prompt-engineering","title":"\u2728 Universe of Pretrained LLMs and Prompt Engineering","text":"<p>In this session, we will introduce various pretrained LLMs, encompassing both open source and proprietary options. We will explore different prompt engineering techniques to use pretrained LLMs for different tasks.</p> <p></p> <p>Coming soon...</p>"},{"location":"#applications-of-llms-and-application-development-frameworks","title":"\u2728 Applications of LLMs and Application Development Frameworks","text":"<p>Explore diverse applications of Large Language Models (LLMs) and the frameworks essential for streamlined application development. Uncover how LLMs can revolutionize tasks and leverage frameworks for efficient integration into real-world applications.</p> <p></p> <p>Coming soon...</p>"},{"location":"#training-and-evaluating-llms-on-custom-datasets","title":"\u2728 Training and Evaluating LLMs On Custom Datasets","text":"<p>Delve into the intricacies of training and evaluating Large Language Models (LLMs) on your custom datasets. Gain insights into optimizing performance, fine-tuning, and assessing model effectiveness tailored to your specific data.</p> <p></p>"},{"location":"#optimizing-llms-for-inference-and-deployment-techniques","title":"\u2728 Optimizing LLMs For Inference and Deployment Techniques","text":"<p>Learn techniques to optimize Large Language Models (LLMs) for efficient inference. Explore strategies for seamless deployment, ensuring optimal performance in real-world applications.</p> <p></p> <p>Coming soon...</p>"},{"location":"#open-challenges-with-llms","title":"\u2728 Open Challenges With LLMs","text":"<p>Delve into the dichotomy of small vs large LLMs, navigating production challenges, addressing research hurdles, and understanding the perils associated with the utilization of pretrained LLMs. Explore the evolving landscape of challenges within the realm of Large Language Models.</p> <p></p> <p>Coming soon...</p>"},{"location":"#llm-courses","title":"\u2728 LLM Courses","text":"<p>List of courses to learn LLMs at your own pace.</p> <p>Coming soon...</p>"},{"location":"#contributing","title":"\ud83d\udc81 Contributing","text":"<p>We are on a generous mission to tackle the daunting FOMO in the LLM race. We need your support for technical articles and related video sessions. See our mission for more details.</p> <p>For detailed information on how to contribute, see contribution guide.</p>"},{"location":"#contributors","title":"\ud83c\udf1f Contributors","text":""},{"location":"contribute/CONTRIBUTION/","title":"Contribution Guide","text":"<p>This guide offers general tips to be followed when writing articles for LLM  Seminar Series. Make sure to read it before opening a PR.</p>"},{"location":"contribute/CONTRIBUTION/#how-to-start","title":"How to start?","text":""},{"location":"contribute/CONTRIBUTION/#overview","title":"Overview","text":"<ul> <li> <p>The homepage outlines the topics which are related to each other and can be covered in a single session. </p> </li> <li> <p>A session is generally divided into multiple parts. For example, the first session was an introductory session and thus it was divided in 3 parts,</p> <ol> <li>Emergence of LLMs</li> <li>Building blocks of LLMs</li> <li>Overview of landscape of LLMs. </li> </ol> </li> <li> <p>We would like to have a detailed article about each part.</p> </li> </ul>"},{"location":"contribute/CONTRIBUTION/#step-1-pick-a-topic","title":"Step 1: Pick a topic","text":"<ul> <li>Please go through the homepage and choose a topic of your interest. </li> <li>It can be something which you have already used in the past or it can be something you want to learn next. </li> <li>Following order is not mandatory, e.g., you can pick the last topic first too.</li> </ul>"},{"location":"contribute/CONTRIBUTION/#step-2-create-an-issue","title":"Step 2: Create an issue.","text":"<ul> <li>As soon as you decide the topic of your interest, create a github issue here.</li> <li>Once you create an issue, we will start sharing some useful references and also provide guidance for writing the perfect article.</li> <li>It also helps collaborating on same topic if multiple people are interested by discussing on the same github issue.</li> </ul>"},{"location":"contribute/CONTRIBUTION/#step-3-fork-the-repo","title":"Step 3: Fork the repo","text":"<ul> <li> <p>We recommend using github from day saving your progress.</p> </li> <li> <p>We follow fork and pull request workflow since its a collaborative project as described in the below image.</p> <p></p> </li> <li> <p>In summary,</p> <ul> <li>you should first fork this repo,</li> <li>create a new branch and add your commits in your branch,</li> <li>once you are ready to submit an article; create a pull request.</li> </ul> </li> <li> <p>The detailed steps can be found here: Contributing to a project</p> </li> </ul>"},{"location":"contribute/CONTRIBUTION/#step-5-organizing-the-files","title":"Step 5: Organizing the files","text":"<ul> <li>We prefer a single markdown file for each article, but you can also submit a jupyter notebook.</li> <li>Put new articles under the following dir:<ul> <li><code>llm_seminar_series/session_&lt;session_number&gt;/part_&lt;part_number&gt;_&lt;part_name&gt;/README.md</code> or <code>llm_seminar_series/session_&lt;session_number&gt;/part_&lt;part_number&gt;_&lt;part_name&gt;/&lt;topic_name&gt;.ipynb</code></li> <li>Where <code>session_number</code> can be found from the homepage, while <code>part_number</code> can be inferred from the outline of the session.</li> </ul> </li> <li>If the article uses any additional source files, then put them along the main file. For example, check RLHF.ipynb.</li> </ul>"},{"location":"contribute/CONTRIBUTION/#step-6-follow-a-consistent-style","title":"Step 6: Follow a consistent style","text":"<ul> <li>Maintaining a consistent style across all the articles provides a good reading experience to our readers and often reduces review time.</li> <li>We have provided code and text style guides separately here. </li> <li>You can refer them when crafting that perfect article.</li> <li>Please use the following commands,<ul> <li>Install ruff: <code>pip install -r requirements.txt</code></li> <li>Run auto formatter: <code>ruff format</code></li> <li>Run linter and fix remaining issues manually <code>ruff check</code></li> </ul> </li> </ul>"},{"location":"contribute/CONTRIBUTION/#step-7-run-the-website-locally","title":"Step 7: Run the website locally","text":"<ul> <li>We use mkdocs to host our repo as a static website.</li> <li>All the articles will be automatically rendered to an html page once merged to main branch.</li> <li>But you can also host this website locally on your machine to visualize the final output which helps solving any formatting issues.</li> <li>Follow these commands to start a local server<ul> <li>Run <code>pip install -r requirements.txt</code> to install all the dependencies.</li> <li>Run <code>mkdocs serve</code> to serve this website locally.</li> <li>You will get this message on your terminal <code>Serving on http://127.0.0.1:8000/</code> from where you can access it.</li> <li>Changes you make in the markdown are reflected immediately on the website which can help you quickly correct any formatting issues.</li> </ul> </li> </ul>"},{"location":"contribute/CONTRIBUTION/#step-8-review-and-approval","title":"Step 8: Review and approval","text":"<ul> <li>Once you are happy with your article, just raise the PR as suggested in step 3.</li> <li>Maintainers of this repo will review the changes and guide you for any modifications if required.</li> <li>Once the PR is approved and merged into main branch, it will be live at the website, Congratulations \ud83c\udf89.</li> </ul>"},{"location":"contribute/CONTRIBUTION/#step-9-youtube-live-session-pre-recorded-video-optional-step","title":"Step 9: YouTube Live session / Pre recorded video (optional step)","text":"<ul> <li>While reading is still the best way to learn something, many prefer watching explanatory videos.</li> <li>We encourage every author to conduct a Youtube Live Session to walk us through the topic using the same article.</li> <li>The link to this video will be put along with the article for future visitors.</li> <li>While live streams are good for interacting with the audience and conduct Q&amp;A, if you prefer to submit a prerecorded video then it's perfectly fine too.</li> </ul> <p>Tip</p> <p>Maintainers of these repo will reach out to you with more details once your article is live.</p>"},{"location":"contribute/OUR_MISSION/","title":"Our Mission","text":""},{"location":"contribute/OUR_MISSION/#the-llm-fomo-and-our-answer-to-it","title":"The LLM FOMO and Our Answer To It !!","text":"<p>TLDR</p> <p>Keeping pace with the rapid advancements in Large Language Models (LLMs) can be a daunting task. This project aims to address this \"Fear Of Missing Out\" (FOMO) by organizing information through a comprehensive mind map.</p> <p>The LLM Boom and Knowledge Explosion</p> <p>Since OpenAI's release of ChatGPT, the LLM field has undergone explosive growth. The research community is constantly innovating, with new ideas emerging on a daily basis. This rapid pace can be overwhelming, with several new LLMs and associated research areas like prompting techniques, fine-tuning methods, evaluation benchmarks, and context window solutions appearing frequently. While exciting for the AI community, it can also lead to a sense of FOMO. </p> <p>Traditionally, keeping up with every new method can be impractical. We propose a more efficient approach: a hierarchical mind map.</p> <p>Organizing the LLM Landscape with a Mind Map</p> <p>Despite the constant stream of new methods, they often fall under existing categories. By organizing this information hierarchically, understanding new methods and their purpose becomes significantly easier.</p> <p>For example, RLHF gained popularity after ChatGPT. Recently other terms such PPO, DPO, and ORPO have also emerged. Can you guess their purpose?</p> <p>The mind map below illustrates these methods as advancements upon each other for fine-tuning LLMs to human preferences.</p> <pre><code>LLM\n \u2514\u2500\u2500 Training_LLM\n     \u2514\u2500\u2500 Supervised_Fine-tuning\n          \u2514\u2500\u2500 Fine-tuning_LLMs_to_Human_Preferences\n               \u251c\u2500\u2500 RLHF\n               \u251c\u2500\u2500 DDPG\n               \u251c\u2500\u2500 PPO\n               \u2514\u2500\u2500 ORPO\n</code></pre> <p>Previously seemingly random names can be instantly understandable when organized hierarchically. Here, all methods represent advancements in fine-tuning LLMs based on human preference.</p> <p>Our Goal: A Continuously Updated LLM Knowledge Base</p> <p>Our objective is to curate and maintain an up-to-date mind map of emerging LLM techniques. This allows you to encounter a new concept (through a LinkedIn post or tweet) and instantly grasp its essence within the context of existing techniques in the mind map.</p> <p>Note</p> <p>As maintainers, we ensure the accuracy and freshness of the mind map and session outline.</p>"},{"location":"contribute/OUR_MISSION/#teaching-is-learning-twice-we-need-your-support","title":"Teaching is Learning Twice: We need your support!!","text":"<p>TLDR</p> <p>We've combined relevant topics from the mind map and tuned them into multi part sessions. We welcome your contributions to create detailed articles and videos for each part.</p> <p>Organizing new research through a mind map is just the first step. True understanding requires deeper exploration. We believe that teaching a concept reinforces learning it. Here's where you come in.</p> <p>On the repository's homepage, you'll find related topics from the mind map organized into session series, each potentially divided into multiple parts. We aim to publish technical blog posts on the website and explanatory videos on YouTube channel for each part, and we highly encourage your contributions.</p> <p>How You Benefit ?</p> <ul> <li>Let's say you choose to write an article on \"Semantic Caching for LLMs\" for session 5, part 3. This process involves in-depth independent learning followed by article creation. </li> <li>The article will be added to the repository and published on the website. Additionally, we'll host a live seminar where you present the article. The live stream will be recorded on YouTube for future reference. </li> <li>By participating in this learn-and-publish cycle, you can continuously update your knowledge with the latest llm work.</li> </ul> <p>How Others Benefit ?</p> <p>Imagine a larger community contributing in this way. Collectively, these smaller contributions can cover a vast range of LLM topics in one central location. This will establish a single, authoritative source for understanding the vast LLM landscape.</p> <ul> <li> <p>Practitioners can leverage this knowledge base when building LLM-based tools. For instance, an engineer considering local LLM deployment can find clear outlines of available options and detailed explanations within relevant sessions. This collaborative approach saves everyone valuable time.</p> </li> <li> <p>Researchers can use the mind map to track progress within specific areas and identify potential areas for improvement in the hierarchy, such as developing novel prompting techniques that complement existing methods.</p> </li> </ul> <p>If our vision aligns with yours, join us! Pick a topic that interests you and become part of this generous effort. </p> <p>Please follow the contribution guide for more details.</p>"},{"location":"contribute/STYLE_GUIDES/","title":"Style Guide","text":"<p>We recommend referring to the provided text and code style for all the articles.</p>"},{"location":"contribute/STYLE_GUIDES/#text-style","title":"Text Style","text":""},{"location":"contribute/STYLE_GUIDES/#length","title":"Length","text":"<p>Examples should be clear and detailed, but not overly verbose. You can add as much text content as you want, as long as each additional sentence / paragraph provides useful information that helps with understanding the example. Never use any \"filler\" content.</p>"},{"location":"contribute/STYLE_GUIDES/#style","title":"Style","text":"<ul> <li>Use present tense (\"We present... we implement...\")</li> <li>Always define abbreviations / acronyms the first time you use them (\"We implement a Graph Attention Network (GAT)...\")</li> <li>All and any sentence should convey a useful idea; avoid filler at all costs.</li> </ul>"},{"location":"contribute/STYLE_GUIDES/#proofreading","title":"Proofreading","text":"<p>Make sure to proofread your text paragraphs to avoid typos. Every sentence should start with a capital letter and should end with a period. This applies to code comments as well.</p>"},{"location":"contribute/STYLE_GUIDES/#introduction-and-conclusion","title":"Introduction and conclusion","text":"<p>There should be an introduction that explains what the reader should expect to find in the example, and why it is useful/interesting. If the example presents a specific technique, the introduction should also include an overview of the technique as well as links to external references. There should be a conclusion section that recapitulates key takeaways from the example, and offers pointers to next steps.</p>"},{"location":"contribute/STYLE_GUIDES/#code-elements","title":"Code elements","text":"<p>All code keywords should be formatted with backticks, e.g. <code>like_this</code> (standard Markdown code formatting).</p> <p>When referring to a function or method name, it should be followed with parens, like this: <code>my_function()</code> or <code>my_method()</code>.</p>"},{"location":"contribute/STYLE_GUIDES/#line-length","title":"Line length","text":"<p>Keep text lines relatively short (about 80 characters), unless it's a link.</p>"},{"location":"contribute/STYLE_GUIDES/#markdown-links","title":"Markdown links","text":"<p>Each markdown link should fit on a single line, unbroken, like this:</p> <pre><code>Here's a link:\n\n[This is the link text](https://github.com/keras-team/keras-io/blob/master/contributor_guide.md)\n</code></pre> <p>Do not break the link like this (or in any other way):</p> <pre><code>[This is the link text](\n    https://github.com/keras-team/keras-io/blob/master/contributor_guide.md)\n</code></pre>"},{"location":"contribute/STYLE_GUIDES/#markdown-lists","title":"Markdown lists","text":"<p>There should be a line break before the first item in any list, e.g.</p> <p>This is good:</p> <pre><code>Here's a list:\n\n- First item\n- Second item\n</code></pre> <p>This is bad:</p> <pre><code>Here's a badly formatted list:\n- First item\n- Second item\n</code></pre>"},{"location":"contribute/STYLE_GUIDES/#code-style","title":"Code Style","text":"<p>We prefer Google Python Style Guide for code.</p>"},{"location":"contribute/STYLE_GUIDES/#variable-names","title":"Variable names","text":"<p>Make sure to use fully-spelled out variable names. Do not use single-letter variable names. Do not use abbreviations unless they're completely obvious (e.g. <code>num_layers</code> is ok).</p> <p>This is bad:</p> <pre><code>m = get_model(u=32, d=0.5)\n</code></pre> <p>This is good:</p> <pre><code>model = get_model(units=32, dropout_rate=0.5)\n</code></pre>"},{"location":"contribute/STYLE_GUIDES/#imports","title":"Imports","text":"<p>Import modules, not individual objects. In particular, don't import individual layers. Typically you should import the following:</p> <pre><code>import tensorflow as tf\nimport keras\nfrom keras import layers\n</code></pre> <p>Then access objects from these modules:</p> <pre><code>tf.Variable(...)\ntf.reshape(...)\nkeras.Input(...)\nkeras.Model(...)\nkeras.optimizers.Adam(...)\nlayers.Layer(...)\nlayers.Conv2D(...)\n</code></pre> <p>Read Imports formatting section for more details on ordering the imports.</p>"},{"location":"contribute/STYLE_GUIDES/#docstrings","title":"Docstrings","text":"<p>A docstring should give enough information to write a call to the function without reading the function\u2019s code. </p> <p>A docstring is mandatory for every function that has one or more of the following properties:</p> <ul> <li>being part of the public API</li> <li>nontrivial size</li> <li>non-obvious logic</li> </ul> <p>The docstring may be descriptive-style (<code>\"\"\"Fetches rows from a Bigtable.\"\"\"</code>) or imperative-style (<code>\"\"\"Fetch rows from a Bigtable.\"\"\"</code>), but the style should be consistent within a file.</p> <p>Certain aspects of a function should be documented in special sections such as <code>Args</code>, <code>Returns</code> and <code>Raises</code>. These sections can be omitted in cases where the function\u2019s name and signature are informative enough that it can be aptly described using a one-line docstring.</p> <pre><code>def fetch_smalltable_rows(\n    table_handle: smalltable.Table,\n    keys: Sequence[bytes | str],\n    require_all_keys: bool = False,\n) -&gt; Mapping[bytes, tuple[str, ...]]:\n    \"\"\"Fetches rows from a Smalltable.\n\n    Retrieves rows pertaining to the given keys from the Table instance\n    represented by table_handle.  String keys will be UTF-8 encoded.\n\n    Args:\n        table_handle: An open smalltable.Table instance.\n        keys: A sequence of strings representing the key of each table\n          row to fetch.  String keys will be UTF-8 encoded.\n        require_all_keys: If True only rows with values set for all keys will be\n          returned.\n\n    Returns:\n        A dict mapping keys to the corresponding table row data\n        fetched. Each row is represented as a tuple of strings. For\n        example:\n\n        {b'Serak': ('Rigel VII', 'Preparer'),\n         b'Zim': ('Irk', 'Invader'),\n         b'Lrrr': ('Omicron Persei 8', 'Emperor')}\n\n        Returned keys are always bytes.  If a key from the keys argument is\n        missing from the dictionary, then that row was not found in the\n        table (and require_all_keys must have been False).\n\n    Raises:\n        IOError: An error occurred accessing the smalltable.\n    \"\"\"\n</code></pre>"},{"location":"contribute/STYLE_GUIDES/#block-and-inline-comments","title":"Block and Inline Comments","text":"<p>Complicated operations get a few lines of comments before the operations commence. Non-obvious ones get comments at the end of the line.</p> <pre><code># We use a weighted dictionary search to find out where i is in\n# the array.  We extrapolate position based on the largest num\n# in the array and the array size and then do binary search to\n# get the exact number.\n\nif i &amp; (i-1) == 0:  # True if i is 0 or a power of 2.\n</code></pre> <p>On the other hand, never describe the code. Assume the person reading the code knows Python (though not what you\u2019re trying to do) better than you do</p> <pre><code># BAD COMMENT: Now go through the b array and make sure whenever i occurs\n# the next element is i+1\n</code></pre>"},{"location":"contribute/STYLE_GUIDES/#punctuation-spelling-and-grammar","title":"Punctuation, Spelling, and Grammar","text":"<p>Pay attention to punctuation, spelling, and grammar; it is easier to read well-written comments than badly written ones.</p> <p>Comments should be as readable as narrative text, with proper capitalization and punctuation. In many cases, complete sentences are more readable than sentence fragments. Shorter comments, such as comments at the end of a line of code, can sometimes be less formal, but you should be consistent with your style.</p>"},{"location":"session_1/","title":"Session 1 - Emergence, Fundamentals and Landscape of LLMs","text":"<p>Covers important building blocks of what we call an LLM today, where they came from, etc. and then we'll dive into the deep universe that has sprung to life around these LLMs.</p> <p>This session is aimed to help:</p> <ul> <li>People who are new to LLMs</li> <li>People who have  just started working on them</li> <li>People who are working on different use cases surrounding LLMs and need a roadmap.</li> </ul>"},{"location":"session_1/#outline","title":"Outline","text":"<ul> <li>Part 1: Emergence of LLMs</li> <li>Part 2: Building Blocks of LLMs</li> <li>Part 3: Landscape of LLMs</li> </ul>"},{"location":"session_1/#session-details","title":"Session details:","text":"<ul> <li>Date: 6 Dec, 2023</li> <li>Speakers: Hetul Patel, Manish Gupta</li> <li>Location: Infocusp Innovations LLP., Ahmedabad</li> </ul>"},{"location":"session_1/#material","title":"Material","text":"<ul> <li>Recording: TODO</li> <li>Presentation: LLM Seminar Series - Session 1</li> </ul>"},{"location":"session_1/part_1_emergence_of_llms/","title":"The Emergence of LLMs","text":"<p>Author: Hetul Patel | Published on: 6 Dec, 2023</p> <pre><code>timeline\n        title Brief History of NLP\n        section 1967\n          MIT\u2019s Eliza (First Chatbot) : \ud83d\udc4c\ud83c\udffc Groundbreaking human-computer interaction : \ud83d\udc4e\ud83c\udffc Limited contextual understanding\n        section 1986\n          Recurrent Neural Networks (RNNs) : \ud83d\udc4c\ud83c\udffc Memory for sequences : \ud83d\udc4e\ud83c\udffc Vanishing gradient for long sentences\n        section 1997\n          Long Short-Term Memory (LSTM) : \ud83d\udc4c\ud83c\udffc Selective ability to memorize or forget, retained long-term dependencies : \ud83d\udc4e\ud83c\udffc Complexity due to 3 different gates\n        section 2014\n          Gated Recurrent Units (GRUs) : \ud83d\udc4c\ud83c\udffc Simplified gating, efficient using reset and update gates : \ud83d\udc4e\ud83c\udffc Limited contextual understanding for long sequences\n          Attention Mechanism: \ud83d\udc4c\ud83c\udffc Dynamic sequence processing, better context retention, offered fresh perspective : \ud83d\udc4e\ud83c\udffc Increased computational complexity\n        section 2017\n          Transformer Architecture : \ud83d\udc4c\ud83c\udffc Parallel sequence processing through multi-head attention : \ud83d\udc4e\ud83c\udffc High computational demand. Due to their size and complexity</code></pre> <pre><code>timeline\n        title Building Upon The Transformers\n        section 2018\n          OpenAI\u2019s GPT-1, Google's BERT Model : \ud83d\udc4c\ud83c\udffc Bert - bidirectional encoder only &lt;br&gt;  GPT - unidirectional, decoder only : \ud83d\udc4e\ud83c\udffc Requires task specific fine-tuning\n        title Building Upon The Transformers\n        section 2019\n          OpenAI's GPT-2, Google\u2019s T5 : \ud83d\udc4c\ud83c\udffc Multi task solving, massive amount of compressed knowledge e.g. GPT-2 (40B data), T5 (7TB data) : \ud83d\udc4e\ud83c\udffc Model size, training complexity\n        section 2020\n          OpenAI's GPT-3 : \ud83d\udc4c\ud83c\udffc Unprecedented versatility, Few shot learning : \ud83d\udc4e\ud83c\udffc Enormous computational requirements, ethical concerns\n        section 2022\n          OpenAI's InstructGPT : \ud83d\udc4c\ud83c\udffc Learn from human feedback during training to follow human instructions better : \ud83d\udc4e\ud83c\udffc Tailored for instructions oriented tasks. Not suitable for natural, dynamic conversation\n          ChatGPT : \ud83d\udc4c\ud83c\udffc Sibling of InstructGPT, optimized for conversations : \ud83d\udc4e\ud83c\udffc Works only with textual data, prone to hallucination, limited knowledge of world upto 2022\n        section 2023\n          GPT-4 : \ud83d\udc4c\ud83c\udffc Handles both text and image, human level on various benchmarks, allows integration of external tools such as web-browsing and code-interpreter : \ud83d\udc4e\ud83c\udffc Lacks other modalities</code></pre>"},{"location":"session_1/part_2_building_blocks_of_llms/","title":"Building Blocks of LLMs","text":"<p>TODO</p>"},{"location":"session_1/part_3_landscape_of_llms/","title":"Landscape of LLMs","text":"<p>Author: Hetul Patel | Published on: 6 Dec, 2023</p> <p></p>"},{"location":"session_1/part_3_landscape_of_llms/#pretrained-llms","title":"Pretrained LLMs","text":""},{"location":"session_1/part_3_landscape_of_llms/#opensource-llms","title":"Opensource LLMs","text":"References <ul> <li> <p>Can we stop relying on proprietary LLMs to evaluate open LLMs?</p> <p><code>Evaluation</code> <code>Open LLM</code> <code>Proprietary LLM</code> <code>GPT-4</code> <code>Feedback Collection dataset</code> <code>Prometheus model</code></p> <p>Using proprietary LLMs like GPT-4 to evaluate open LLMs has limitations. The Feedback Collection dataset and the Prometheus model aim to close the gap between open and closed models by providing a way to evaluate open LLMs without relying on proprietary models.</p> </li> <li> <p>MosaicML releases MPT-30B, a 30 billion parameter LLM that outperforms GPT-3</p> <p><code>LLM</code> <code>Open Source</code> <code>Machine Learning</code> <code>Artificial Intelligence</code></p> <p>MosaicML has released MPT-30B, a 30 billion parameter LLM that outperforms the original GPT-3 175 billion parameter model. It is fully open source for commercial use and comes with two fine-tuned variants: MPT-30B-Instruct and MPT-30B-Chat. MPT-30B-Chat is available to play with on HuggingFace, powered by MosaicML Inference. If you want to start using MPT-30B in production, you can customize and deploy it using MosaicML Training and MosaicML Inference.</p> </li> <li> <p>OpenChat Surpasses ChatGPT Performance With Open-Source Model</p> <p><code>LLM</code> <code>OpenAI</code> <code>ChatGPT</code> <code>NLP</code> <code>Machine Learning</code></p> <p>OpenChat has developed a new language model, Orca, that outperforms ChatGPT on the Vicuna benchmark. Orca was trained on a smaller dataset than ChatGPT, but achieved better performance by using a more efficient training method. OpenChat has made Orca open-source, so that other researchers can build on its success.</p> </li> <li> <p>The Latest Advancements in Large Language Models: Unveiling Llama 2, Code Llama, and More</p> <p><code>LLM</code> <code>Llama 2</code> <code>Code Llama</code> <code>GPT-4</code> <code>OpenAI</code> <code>Finetuning</code> <code>Transformer-based LLMs</code> <code>NeurIPS LLM Efficiency Challenge</code></p> <p>The article discusses the latest advancements in large language models (LLMs), including the release of Meta's Llama 2 and Code Llama models, the leaked GPT-4 model details, OpenAI's new finetuning API, and the NeurIPS LLM Efficiency Challenge. It provides a comprehensive overview of the key features, capabilities, and potential applications of these models, while also highlighting ongoing challenges and debates in the field of LLMs.</p> </li> <li> <p>Announcing Mistral 7B: The Most Powerful Language Model For Its Size</p> <p><code>language-models</code> <code>machine-learning</code> <code>artificial-intelligence</code></p> <p>The Mistral AI team has released Mistral 7B, a 7.3B parameter language model that outperforms Llama 2 13B on all metrics. It is easy to fine-tune on any task and is released under the Apache 2.0 license.</p> </li> <li> <p>Hugging Face Unveils Zephyr-7b: A State-of-the-Art 7B Chatbot</p> <p><code>LLM</code> <code>Chatbot</code> <code>Natural Language Processing</code> <code>Artificial Intelligence</code></p> <p>Hugging Face has released Zephyr-7b, a 7B chatbot that outperforms other models in its class on the MT Bench and Open LLM Leaderboard. The model was trained using a combination of instruction fine-tuning and Direct Preference Optimization on publicly available datasets. It is available to try out on the Hugging Face website.</p> </li> <li> <p>LaMini-LM: Can Small Language Models Compete with Large Ones?</p> <p><code>language models</code> <code>parameter scale</code> <code>computational requirements</code> <code>LaMini-LM</code> <code>distilled instructions</code></p> <p>LaMini-LM is a small language model with a huge amount of distilled instructions. It is designed to achieve impressive results with a smaller model locally. In this article, we will delve into the details of LaMini-LM and see how tiny computational requirements the model asks for.</p> </li> <li> <p>Open Source LLaMA 13B Released with Full Commercial Usage Rights</p> <p><code>Open Source LLaMA</code> <code>RedPajama Dataset</code> <code>SlimPajama Dataset</code> <code>Code Generation</code> <code>Commercial Usage</code> <code>Energy Efficiency</code></p> <p>OpenLM research has released a fully open source version of the LLaMA 13B model, trained on the RedPajama dataset. The model weights are available in both Jax and PyTorch. The model is not ideal for code generation due to its treatment of empty spaces, but it remains one of the best open source models for building on top of. The authors are considering training future releases on the SlimPajama dataset, which is a cleaned version of the RedPajama dataset with 49% smaller size.</p> </li> <li> <p>Meet Notus-7B: Data Curation and Open Science go a long way in shaping AI's future</p> <p><code>Open Source LLM</code> <code>RLHF</code> <code>DPO</code></p> <p>LLama 1 &amp; 2 opened the floodgates of open source LLMs. MistralAI released the most powerful 7B base LLM remotely inspired by the success of LLama 2. HuggingFace H4 released Zephyr trained on on a mix of publicly available, synthetic datasets using DPO. TsinghuaNLP released the UltraChat dataset, a large-scale, multi-round dialogue dataset. OpenBMB released the UltraFeedback dataset, a large-scale, fine-grained, diverse preference dataset for RLHF and DPO. Huggingface H4 team fine-tuned Zephyr using UltraChat (supervised fine tuning) and UltraFeedback (DPO for alignment). ArgillaIO fixed some data issues and improved on Zephyr to release Notus-7B.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#prompt-engineering","title":"Prompt Engineering","text":"References <ul> <li> <p>Prompt Engineering Guide</p> <p><code>Prompt Engineering</code> </p> <p>Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language models (LLMs).</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#training-llms","title":"Training LLMs","text":"References <ul> <li> <p>Efficient Deep Learning Optimization Libraries for Large Language Model Training</p> <p><code>DeepSpeed</code> <code>Megatron-DeepSpeed</code> <code>FairScale</code> <code>Megatron-LM</code> <code>Colossal-AI</code> <code>BMTrain</code> <code>Mesh TensorFlow</code> <code>max text</code> <code>Alpa</code> <code>GPT-NeoX</code></p> <p>This article provides an overview of various deep learning optimization libraries that can simplify and optimize the training process for large language models. These libraries offer features such as distributed training, model parallelism, and efficient training algorithms, enabling researchers and practitioners to achieve better results with less effort.</p> </li> <li> <p>LLM Training Techniques</p> <p><code>Training vs Prompting Engineering</code> <code>Task Diversity for OOD Robustness</code> <code>Self-Instruction for Dataset Generation</code> <code>Self-Consistency for Higher Performance</code> <code>Evaluation</code></p> <p>This MLOps Community podcast with Mark Huang discusses various LLM training techniques, including training vs prompting engineering, task diversity for OOD robustness, self-instruction for dataset generation, self-consistency for higher performance, and evaluation.</p> </li> <li> <p>Deploying RLHF with 0 Annotations: A Case Study</p> <p><code>real-world case-study</code> <code>reducing manual effort</code> <code>RLHF</code> <code>translation quality</code> <code>reward model</code> <code>user-designated pair</code> <code>regression model</code> <code>Allen AI's library RL4LMs</code> <code>T5/Flan-T5</code> <code>HF Trainer</code> <code>Sentence Transformers Cross-Encoders</code></p> <p>This article presents a real-world case study of deploying RLHF with 0 annotations. It describes the challenges faced by a large translation company in SE Asia, and how RLHF was used to reduce manual effort in producing domain-specific vocabulary and robotic translations. The article also discusses the tools and libraries used, and provides a key takeaway for readers.</p> </li> <li> <p>X-LLM: A Framework for Training Multimodal Language Models</p> <p><code>Multimodal Language Models</code> <code>X-LLM</code> <code>Image Captioning</code> <code>Text-to-Speech</code> <code>Multimodal Question Answering</code></p> <p>The paper proposes a new framework, X-LLM, for training multimodal language models. X-LLM consists of three main components: single-modal encoders, X2L interfaces, and a large language model (LLM). The authors evaluate X-LLM on a variety of tasks and show that it achieves state-of-the-art results.</p> </li> <li> <p>TRL: A Full-Stack Transformer Language Model with Reinforcement Learning</p> <p><code>Reinforcement Learning</code> <code>Transformer Language Models</code> <code>Supervised Fine-tuning</code> <code>Reward Modeling</code> <code>Proximal Policy Optimization</code></p> <p>TRL is a full-stack library that provides tools for training transformer language models and stable diffusion models with Reinforcement Learning. It is built on top of the transformers library by \ud83e\udd17 Hugging Face and supports most decoder and encoder-decoder architectures.</p> </li> <li> <p>Is Reinforcement Learning Really Necessary for Large Language Models?</p> <p><code>language models</code> <code>reinforcement learning</code> <code>direct preference optimization</code></p> <p>The paper \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\" introduces a novel algorithm that gets rid of the two stages of RL, namely - fitting a reward model, and training a policy to optimize the reward via sampling. This new algorithm, called Direct Preference Optimization (DPO), trains the LLM using a new loss function which encourages it to increase the likelihood of the better completion and decrease the likelihood of the worse completion. DPO has been shown to achieve comparable performance to RL-based methods, but is much simpler to implement and scale.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#supervised-finetuning","title":"Supervised Finetuning","text":"References <ul> <li> <p>Fine-tuning Llama-2 on your own data</p> <p><code>LLM</code> <code>Fine-tuning</code> <code>Natural Language Processing</code></p> <p>The new script allows for fine-tuning Llama-2 on your own data in just a few lines of code. It handles single/multi-gpu and can even be used to train the 70B model on a single A100 GPU by leveraging 4bit.</p> </li> <li> <p>Fine-tuning LLMs for specific tasks</p> <p><code>LLM</code> <code>fine-tuning</code> <code>performance</code></p> <p>The author of the ReAct paper explores the effects of fine-tuning LLMs on specific tasks. They found that fine-tuning significantly improves performance when using the LLM as an agent. The key is to fine-tune each module to tailor it to specific tasks.</p> </li> <li> <p>A discussion on various LLM fine-tuning techniques</p> <p><code>lora</code> <code>adapter</code> <code>prompt tuning</code> <code>rl based policy finetuning</code></p> <p>The post discusses various LLM fine-tuning techniques. It covers LORA, adapters, prompt tuning and RL based policy finetuning. The discussion revolves around the advantages and disadvantages of each technique and the scenarios where they are most suitable.</p> </li> <li> <p>Fine-tuning Mistral-7b with QLoRA on Google Colab</p> <p><code>LLM</code> <code>Mistral-7b</code> <code>QLoRA</code> <code>Hugging Face</code> <code>TRL</code> <code>PEFT</code></p> <p>The article describes how to fine-tune the Mistral-7b language model using QLoRA on Google Colab. This can be done using the TRL and PEFT tools from the Hugging Face ecosystem. The article also includes links to the Google Colab notebook and a GitHub thread with more information.</p> </li> <li> <p>Instruction-tuning 101</p> <p><code>InstructGPT</code> <code>T0</code> <code>The Turking Test</code> <code>FLAN</code> <code>Natural Instructions</code></p> <p>Instruction-tuning is a method for improving the performance of language models on a given task by providing them with additional instructions. This can be done by either fine-tuning the model on a dataset of instructions or by using a pre-trained model and providing it with instructions at inference time. Instruction-tuning has been shown to be effective for a variety of tasks, including text summarization, question answering, and machine translation.</p> </li> <li> <p>LLM Reasoning Capabilities Improve with Increased Parameters</p> <p><code>reasoning</code> <code>structured outputs</code> <code>fine-tuning</code></p> <p>A survey of llama2-chat models shows that reasoning capabilities improve as the number of parameters increases. However, structured outputs remain a challenge. This suggests that fine-tuning for better structured data extraction could potentially help.</p> </li> <li> <p>Finetuning Overview</p> <p><code>Finetuning</code> <code>In-context learning</code> <code>Retrieval augmentation</code> <code>Embedding finetuning</code> <code>LLM finetuning</code> <code>LlamaIndex integrations</code></p> <p>Finetuning a model involves updating the model itself over a set of data to improve the model in various ways. This can include improving the quality of outputs, reducing hallucinations, memorizing more data holistically, and reducing latency/cost. The core of our toolkit revolves around in-context learning / retrieval augmentation, which involves using the models in inference mode and not training the models themselves. While finetuning can be also used to \u201caugment\u201d a model with external data, finetuning can complement retrieval augmentation in a variety of ways.</p> </li> <li> <p>T-Few Finetuning: Efficient Training and Scalable Serving of Large Language Models</p> <p><code>large language models</code> <code>finetuning</code> <code>T-Few</code> <code>training efficiency</code> <code>serving scalability</code></p> <p>T-Few finetuning is a technique that selectively updates only a fraction of the model's weights, thus reducing training time and computational resources. It also enables model stacking, which allows for the concurrent inference of multiple finetunes, maximizing GPU utilization and improving serving scalability.</p> </li> <li> <p>How to Fine-tune Llama 2 Embeddings for Better Retrieval Performance</p> <p><code>LLM</code> <code>RAG</code> <code>Embedding Finetuning</code> <code>LlamaIndex</code></p> <p>This article provides a step-by-step guide on how to fine-tune Llama 2 embeddings for better retrieval performance in RAG systems. The guide includes instructions on how to generate training data, fine-tune the embedding model, and evaluate the performance of the fine-tuned model.</p> </li> <li> <p>RL4LMs: A Modular RL Library for Fine-Tuning Language Models to Human Preferences</p> <p><code>language models</code> <code>reinforcement learning</code> <code>natural language processing</code></p> <p>RL4LMs is a modular RL library for fine-tuning language models to human preferences. It provides easily customizable building blocks for training language models, including implementations of on-policy algorithms, reward functions, metrics, datasets, and LM-based actor-critic policies.</p> </li> <li> <p>Exploring Alternatives to RLHF for Fine-Tuning Large Language Models</p> <p><code>Large Language Models</code> <code>Supervised Fine-Tuning</code> <code>Reinforcement Learning from Human Feedback</code> <code>Direct Preference Optimization</code> <code>Chain of Hindsight</code></p> <p>This blog explores alternatives to Reinforcement Learning from Human Feedback (RLHF) for fine-tuning large language models. The alternatives discussed include supervised fine-tuning and direct preference optimization. The blog also provides a hands-on guide to preparing human preference data and using the Transformers Reinforcement Learning library to fine-tune a large language model using direct preference optimization.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#evaluating-llms","title":"Evaluating LLMs","text":"References <ul> <li> <p>LMFlow Benchmark: An Automatic Evaluation Framework for Open-Source LLMs</p> <p><code>LLM Evaluation</code> <code>Chatbot Arena</code> <code>GPT-4</code> <code>LMFlow Benchmark</code></p> <p>The paper introduces LMFlow benchmark, a new benchmark which provides a cheap and easy-to-use evaluation framework that can help reflect different aspects of LLMs.</p> </li> <li> <p>Evaluating LLM Performance</p> <p><code>LLM Evaluation</code> <code>RAG</code> <code>Hallucinations</code> <code>Metrics</code></p> <p>This article discusses various techniques for evaluating LLM performance, including hallucination detection and metrics-based approaches. It also provides a framework for optimizing LLM performance using RAG and fine-tuning.</p> </li> <li> <p>A Metrics-First Approach to LLM Evaluation</p> <p><code>LLM Evaluation</code> <code>Human Evaluation</code> <code>Traditional Metrics</code> <code>Galileo Metrics</code></p> <p>The industry has started adopting LLMs for various applications, but evaluating their performance is challenging. Human evaluation is costly and prone to errors, traditional metrics have poor correlations with human judgment, and reliable benchmarks are absent. Galileo has built metrics to help evaluate LLMs in minutes instead of days.</p> </li> <li> <p>Evaluation Driven Development for LLM Apps</p> <p><code>Evaluation Driven Development</code> <code>LLM</code> <code>EDD</code> <code>Stochastic nature of LLMs</code> <code>LlamaIndex</code> <code>Retrieval methods</code> <code>Comparing LLMs</code></p> <p>The article discusses the importance of Evaluation Driven Development (EDD) for building LLM apps. It provides a step-by-step guide to EDD, including defining evaluation metrics, defining an evaluation dataset, and trying out different approaches. The article also highlights the importance of EDD for mitigating the risks associated with the stochastic nature of LLMs. Finally, the article provides links to additional resources on EDD.</p> </li> <li> <p>How to Evaluate Chatbots with Large Language Models</p> <p><code>Chatbots</code> <code>LLM</code> <code>RAG</code> <code>Evaluation</code> <code>MLflow</code></p> <p>This article explores how to evaluate chatbots with large language models (LLMs). It discusses the use of LLMs as judges for automated evaluation, and provides best practices for using LLM judges. The article also discusses the importance of using use-case-specific benchmarks for evaluation.</p> </li> <li> <p>How to Monitor NDCG for Ranking Models in Production</p> <p><code>Ranking models</code> <code>NDCG</code> <code>ML observability</code> <code>Model monitoring</code> <code>Machine learning</code></p> <p>This article provides a comprehensive guide to monitoring Normalized Discounted Cumulative Gain (NDCG) for ranking models in production. It covers the intuition behind NDCG, its calculation, and how it can be used to evaluate the performance of ranking models. Additionally, the article discusses the challenges of maintaining ranking models in production and how ML observability can help.</p> </li> <li> <p>Index Metrics for Evaluating Recommender System Performance</p> <p><code>Recommender Systems</code> <code>Evaluation Metrics</code> <code>Hit Ratio</code> <code>MRR</code> <code>Precision</code> <code>Recall</code> <code>MAP</code> <code>NDCG</code></p> <p>Recommender systems output a ranking list of items. Hit ratio, MRR, Precision, Recall, MAP, NDCG are commonly used metrics to evaluate the performance of recommender systems.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#llms-deployment","title":"LLMs Deployment","text":"References <ul> <li> <p>Model Serving Frameworks for 2023</p> <p><code>Model Serving</code> <code>AI</code> <code>Machine Learning</code> <code>MLOps</code></p> <p>The article provides a comprehensive list of model serving frameworks for AI applications in 2023. It highlights the benefits and features of each framework, including BentoML, Jina, and Torchserve, and emphasizes their importance in the MLOps process.</p> </li> <li> <p>vLLM: A High-Throughput Library for Large Language Model Serving</p> <p><code>LLM</code> <code>machine learning</code> <code>artificial intelligence</code> <code>natural language processing</code></p> <p>vLLM is an open-source library for fast LLM inference and serving. It utilizes PagedAttention, a new attention algorithm that effectively manages attention keys and values. vLLM equipped with PagedAttention redefines the new state of the art in LLM serving: it delivers up to 24x higher throughput than HuggingFace Transformers, without requiring any model architecture changes.</p> </li> <li> <p>How to Optimize Latency for Open Source Language Models</p> <p><code>Optimization</code> <code>Latency</code> <code>LLM</code> <code>Model Serving</code> <code>Inference</code></p> <p>This study explores various approaches to optimizing latency for open-source LLMs. The author evaluates the effectiveness of different tools and techniques, including CTranslate2, TGI, bitsandbytes, AutoGPTQ, ExLlama, vLLM, and HuggingFace's hosted inference platform. The results show that vLLM is currently the fastest solution for distributed inference, while HuggingFace's hosted inference platform offers the best performance for single-GPU inference.</p> </li> <li> <p>How to Optimize Large Language Model (LLM) Inference</p> <p><code>Large Language Model</code> <code>LLM</code> <code>Inference</code> <code>Optimization</code> <code>Machine Learning</code></p> <p>This article provides best practices for optimizing LLM inference, including identifying the optimization target, paying attention to the components of latency, utilizing memory bandwidth, batching, and exploring deeper systems optimizations. It also discusses hardware configurations and the importance of data-driven decisions.</p> </li> <li> <p>Text Generation Inference</p> <p><code>HuggingFace</code> <code>LLM</code> <code>Rust</code> <code>Python</code> <code>gRPC</code> <code>Docker</code> <code>CUDA</code> <code>NCCL</code> <code>OpenTelemetry</code> <code>quantization</code></p> <p>Text Generation Inference (TGI) is a toolkit for deploying and serving Large Language Models (LLMs). It implements many features such as optimized models, tensor parallelism, and distributed tracing. TGI can be installed locally or used as a Docker container.</p> </li> <li> <p>Introducing text-embeddings-inference (TEI): A blazing fast server for sentence or document embedding</p> <p><code>Machine Learning</code> <code>Natural Language Processing</code> <code>Text Embeddings</code> <code>Serverless Computing</code></p> <p>TEI is a new server for sentence or document embedding that is optimized for speed and efficiency. It is based on the <code>candle</code> rust backend and does not require torch, making it very small and lightweight. TEI is a step towards real ML serverless and has the potential to make it easier to use multimodal embeddings in production.</p> </li> <li> <p>Text Generation Inference: A Rust, Python, and gRPC toolkit</p> <p><code>HuggingFace</code> <code>Hugging Chat</code> <code>Inference API</code> <code>Inference Endpoint</code> <code>Large Language Models (LLMs)</code> <code>Llama</code> <code>Falcon</code> <code>StarCoder</code> <code>BLOOM</code> <code>GPT-NeoX</code> <code>Open Telemetry</code> <code>Prometheus</code> <code>Tensor Parallelism</code> <code>Server-Sent Events (SSE)</code> <code>transformers.LogitsProcessor</code> <code>Custom Prompt Generation</code> <code>Fine-tuning Support</code></p> <p>Text Generation Inference (TGI) is a toolkit for deploying and serving Large Language Models (LLMs). It supports many features such as simple launcher, production readiness, tensor parallelism, token streaming, continuous batching, optimized transformers code, quantization, watermarking, logits warper, stop sequences, log probabilities, custom prompt generation, and fine-tuning support.</p> </li> <li> <p>LoRAX: The LLM Inference Server that Speaks for the GPUs</p> <p><code>LLM</code> <code>LoRA</code> <code>GPU</code> <code>Cloud</code> <code>Predibase</code></p> <p>LoRAX is a new kind of LLM inference solution designed to make it cost effective and scalable to serve many fine-tuned models in production at once, conserving precious GPUs by dynamically exchanging in and out fine-tuned LoRA models within a single LLM deployment.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#running-llms-locally","title":"Running LLMs Locally","text":"References <ul> <li> <p>Run Large Language Models on Your CPU with Llama.cpp</p> <p><code>LLM</code> <code>Inference</code> <code>CPU</code> <code>GPU</code> <code>ChatGPT</code> <code>Vicuna</code> <code>GPT4ALL</code> <code>Alpaca</code> <code>ggml</code></p> <p>This article explains how to set up llama.cpp on your computer to run large language models on your CPU. It focuses on Vicuna, a chat model behaving like ChatGPT, but also shows how to run llama.cpp for other language models.</p> </li> <li> <p>h2oGPT - 100% Private, 100% Local Chat with a GPT</p> <p><code>LLM</code> <code>h2oGPT</code> <code>Open Source</code> <code>Private</code> <code>Local</code></p> <p>This video shows how to install and use h2oGPT, an open-source large language model (LLM), on a local computer for private, local chat with a GPT.</p> </li> <li> <p>Run Large Language Models on Your Own Computer with llama.cpp</p> <p><code>Large Language Models</code> <code>Llama.cpp</code> <code>NVIDIA CUDA</code> <code>Ubuntu 22.04</code></p> <p>This blog post provides a step-by-step guide for running the Llama-2 7B model using llama.cpp, with NVIDIA CUDA and Ubuntu 22.04.</p> </li> <li> <p>Get up and running with Llama 2 and other large language models locally</p> <p><code>LLM</code> <code>Ollama</code> <code>Modelfile</code> <code>Docker</code> <code>REST API</code></p> <p>This article provides instructions on how to get up and running with Llama 2 and other large language models locally. It covers topics such as installing Docker, downloading models, customizing prompts, and using the REST API.</p> </li> <li> <p>GPT4All: A Free, Local, Privacy-Aware Chatbot</p> <p><code>privacy</code> <code>local</code> <code>chatbot</code></p> <p>GPT4All is a free-to-use, locally running chatbot that does not require a GPU or internet connection. It is designed to be privacy-aware and does not collect or store any user data.</p> </li> <li> <p>LocalAI: An Open Source OpenAI Alternative</p> <p><code>LLM</code> <code>OpenAI</code> <code>gpt-3</code> <code>localai</code></p> <p>LocalAI is a free, open-source alternative to OpenAI that allows you to run LLMs, generate images, audio, and more locally or on-prem with consumer-grade hardware. It does not require a GPU and supports multiple model families that are compatible with the ggml format.</p> </li> <li> <p>LocalGPT: Chat with your documents on your local device using GPT models</p> <p><code>localgpt</code> <code>gpt-3</code> <code>language-models</code> <code>privacy</code> <code>security</code></p> <p>LocalGPT is an open-source initiative that allows you to converse with your documents without compromising your privacy. With everything running locally, you can be assured that no data ever leaves your computer.</p> </li> <li> <p>Run any Llama 2 locally with gradio UI on GPU or CPU from anywhere (Linux/Windows/Mac)</p> <p><code>GPU</code> <code>CPU</code> <code>Linux</code> <code>Windows</code> <code>Mac</code> <code>Llama 2</code> <code>gradio UI</code> <code>Generative Agents/Apps</code></p> <p>This project enables users to run any Llama 2 model locally with a gradio UI on GPU or CPU from anywhere (Linux/Windows/Mac). It uses <code>llama2-wrapper</code> as the local llama2 backend for Generative Agents/Apps.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#semantic-cache-for-llms","title":"Semantic Cache for LLMs","text":"References <ul> <li> <p>GPTCache: Semantic Cache for LLMs</p> <p><code>LLM</code> <code>Semantic Caching</code> <code>LangChain</code> <code>Llama Index</code></p> <p>GPTCache is a semantic cache for LLMs that helps reduce the cost and latency of LLM API calls. It uses embedding algorithms to convert queries into embeddings and uses a vector store for similarity search on these embeddings. This allows GPTCache to identify and retrieve similar or related queries from the cache storage, thereby increasing cache hit probability and enhancing overall caching efficiency.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#llms-inference-optimisation","title":"LLMs Inference Optimisation","text":""},{"location":"session_1/part_3_landscape_of_llms/#llm-quantization","title":"LLM Quantization","text":"References <ul> <li> <p>BitNet: Scaling 1-bit Transformers for Large Language Models</p> <p><code>Transformers</code> <code>Quantization</code> <code>LLM</code></p> <p>BitNet is a scalable and stable 1-bit Transformer architecture designed for large language models. It achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.</p> </li> <li> <p>HuggingFace: An Overview of Natively Supported Quantization Schemes in Transformers</p> <p><code>HuggingFace</code> <code>Transformers</code> <code>Quantization</code></p> <p>The article provides an overview of natively supported quantization schemes in Transformers, including bitsandbytes and GPTQ. It also discusses the relation between bitsandbytes and GPTQ, and compares the performance of GPTQ with bitsandbytes nf4.</p> </li> <li> <p>Hugging Face Optimum GPTQ Quantization</p> <p><code>Hugging Face</code> <code>Optimum</code> <code>GPTQ</code> <code>Quantization</code> <code>LLM</code> <code>NLP</code></p> <p>This blog post introduces GPTQ quantization, a method to compress GPT models by reducing the number of bits needed to store each weight. It also provides a step-by-step tutorial on how to quantize a GPT model using the Hugging Face Optimum library.</p> </li> <li> <p>SqueezeLLM: Efficient LLM Serving with Dense-and-Sparse Quantization</p> <p><code>Model Compression</code> <code>Quantization</code> <code>Efficient Serving</code></p> <p>SqueezeLLM is a post-training quantization framework that incorporates a new method called Dense-and-Sparse Quantization to enable efficient LLM serving. This method splits weight matrices into two components: a dense component that can be heavily quantized without affecting model performance, and a sparse part that preserves sensitive and outlier parts of the weight matrices. With this approach, SqueezeLLM is able to serve larger models with smaller memory footprint, the same latency, and yet higher accuracy and quality.</p> </li> <li> <p>SqueezeLLM: Achieving 3-bit Quantization for LLM Inference Acceleration</p> <p><code>Post-Training Quantisation (PQT)</code> <code>Non-Uniform Quantization</code> <code>Dense and Sparse Quantization</code> <code>Memory Bottlenecked Operations</code> <code>GPU Memory Optimization</code> <code>Model Compression</code> <code>LLM Inference Acceleration</code></p> <p>The paper proposes SqueezeLLM, a novel Post-Training Quantisation (PQT) technique that achieves 3-bit quantization for LLM inference acceleration. It introduces non-uniform quantization and dense and sparse quantization to address memory bottlenecks and achieve 230% speedup in inference. The paper also compares SqueezeLLM with other quantization techniques and demonstrates its superior performance in terms of compression and accuracy.</p> </li> <li> <p>New Research Paper: Sparse Quantized Representation for Efficient Large Language Model Compression</p> <p><code>LLM Compression</code> <code>SpQR</code> <code>Quantization</code> <code>Falcon</code> <code>LLaMA</code></p> <p>A new research paper introduces Sparse Quantized Representation (SpQR), a new compression format and quantization technique that enables near-lossless compression of LLMs down to 3-4 bits per parameter. This technique works by recognizing and isolating outlier weights that cause large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits. The authors claim that SpQR can achieve relative accuracy losses of less than 1% in perplexity for highly accurate LLMs like Falcon and LLaMA.</p> </li> <li> <p>Two Cool Releases from Last Week in the LLM Domain</p> <p><code>RedPajama Dataset</code> <code>LLM Model Family</code> <code>HELM Benchmark</code></p> <p>Cerebras Systems has released a cleaned and de-duplicated version of the RedPajama Dataset, reducing its size by 49%. Additionally, RedPajama has released a model family of 7B size, including chat, instruction fine-tuned, and base models. The instruction fine-tuned model shows promising performance on the HELM benchmark.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#llms-with-large-context-window","title":"LLMs with Large Context Window","text":"References <ul> <li> <p>How to use 100K context window in LLMs</p> <p><code>LLM Training</code> <code>Model Size</code> <code>Attention Mechanisms</code></p> <p>This article explores techniques to speed up training and inference of LLMs to use large context window up to 100K input tokens. It covers ALiBi positional embedding, Sparse Attention, FlashAttention, Multi-Query attention, Conditional computation, and the use of 80GB A100 GPUs.</p> </li> <li> <p>XGen: A New State-of-the-Art 7B LLM with Standard Dense Attention on Up to 8K Sequence Length</p> <p><code>LLM</code> <code>NLP</code> <code>Machine Learning</code> <code>Artificial Intelligence</code></p> <p>XGen is a new state-of-the-art 7B LLM with standard dense attention on up to 8K sequence length. It achieves comparable or better results than other open-source LLMs of similar model size on standard NLP benchmarks. XGen also shows benefits on long sequence modeling benchmarks and achieves great results on both text and code tasks.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#challenges-with-llms","title":"Challenges with LLMs","text":"References <ul> <li> <p>Challenges in Building LLM Applications for Production</p> <p><code>Consistency</code> <code>Hallucinations</code> <code>Privacy</code> <code>Context Length</code> <code>Data Drift</code> <code>Model Updates and Compatibility</code> <code>LM on the Edge</code> <code>Model Size</code> <code>Non-English Languages</code> <code>Chat vs. Search as an Interface</code> <code>Data Bottleneck</code> <code>Hype Cycles and the Importance of Data</code></p> <p>This talk discusses the challenges in building LLM applications for production. These challenges include consistency, hallucinations, privacy, context length, data drift, model updates and compatibility, LM on the edge, model size, non-English languages, chat vs. search as an interface, data bottleneck, and hype cycles and the importance of data.</p> </li> <li> <p>Open challenges in LLM research</p> <p><code>hallucinations</code> <code>context learning</code> <code>multimodality</code> <code>new architecture</code> <code>GPU alternatives</code> <code>agent usability</code> <code>learning from human preference</code> <code>chat interface efficiency</code> <code>non-English language support</code></p> <p>The article discusses the ten major research directions in the field of LLMs, including reducing and measuring hallucinations, optimizing context length and construction, incorporating other data modalities, making LLMs faster and cheaper, designing new model architectures, developing GPU alternatives, making agents usable, improving learning from human preference, improving the efficiency of the chat interface, and building LLMs for non-English languages.</p> </li> <li> <p>The Perils of Blindly Reusing Pre-trained Language Models</p> <p><code>NLP</code> <code>Transfer Learning</code> <code>Model Analysis</code> <code>WeightWatchers</code></p> <p>Reusing pre-trained language models without careful consideration can lead to negative impacts on downstream tasks due to issues such as over-training, under-training, or over-parameterization. WeightWatchers is an open-source diagnostic tool that can be used to analyze DNNs without access to training or test data, helping to identify potential issues before deployment.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#large-vs-small-langage-models","title":"Large vs Small Langage Models","text":"References <ul> <li> <p>Small language models can outperform LLMs in specific domains</p> <p><code>LLM</code> <code>NLP</code> <code>Machine Learning</code></p> <p>A new LLM trained by Microsoft Research achieves 51% on HumanEval with only 1.3B parameters and 7B tokens training dataset, outperforming much larger LLMs. This suggests that smaller language models can be more effective in specific domains, such as Python code-generation.</p> </li> <li> <p>Are Large Language Models All We Need?</p> <p><code>LLM</code> <code>Model Size</code> <code>Data Quality</code></p> <p>The author discusses the recent trend of focusing on model sizes in the field of LLMs and argues that data quality is often overlooked. They cite the example of phi-1, a 1.3B parameter Transformer-based model by Microsoft, which achieved surprisingly good results. The author concludes that we should pay more attention to data quality when developing LLMs.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#llm-applications","title":"LLM Applications","text":""},{"location":"session_1/part_3_landscape_of_llms/#llms-for-translation","title":"LLMs for Translation","text":"References <ul> <li> <p>ParroT: Enhancing and Regulating Translation Abilities in Chatbots with Open-Source LLMs</p> <p><code>LLM</code> <code>Translation</code> <code>Chatbots</code> <code>ParroT</code></p> <p>The ParroT framework enhances and regulates the translation abilities of chatbots by leveraging open-source LLMs and human-written translation and evaluation data.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#llms-for-mobile-app-developers","title":"LLMs For Mobile App Developers","text":"References <ul> <li> <p>Hugging Face releases tools for Swift developers to incorporate language models in their apps</p> <p><code>Hugging Face</code> <code>Swift</code> <code>transformers</code> <code>Core ML</code> <code>Llama</code> <code>Falcon</code></p> <p>Hugging Face has released a package and tools to help Swift developers incorporate language models in their apps, including swift-transformers, swift-chat, transformers-to-coreml, and ready-to-use LLMs such as Llama 2 7B and Falcon 7B.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#llm-assistants","title":"LLM Assistants","text":"References <ul> <li> <p>Comparing coding assistants</p> <p><code>Rust</code> <code>coding assistants</code> <code>best practices</code></p> <p>The author asks for advice on how to compare coding assistants. They are concerned about using an assistant for Rust because they are not savvy enough to catch certain bugs. Kalyan KS suggests that the author try out Falcoder, a coding assistant that uses the Falcon-7B model and instruction tuning.</p> </li> <li> <p>GPT-Engineer: An AI Agent That Can Write Entire Codebases</p> <p><code>Artificial Intelligence</code> <code>Machine Learning</code> <code>Natural Language Processing</code> <code>Programming</code></p> <p>GPT-Engineer is an AI agent that can write entire codebases with a prompt and learn how you want your code to look. It asks clarifying questions, generates technical specifications, writes all necessary code, and lets you easily add your own reasoning steps, modify, and experiment. With GPT-Engineer, you can finish a coding project in minutes.</p> </li> <li> <p>Introducing AssistGPT: A General Multi-modal Assistant</p> <p><code>Multimodality</code> <code>Language and Code</code> <code>ReAct Agent</code> <code>Planning and Execution</code></p> <p>The paper introduces AssistGPT, a general multi-modal assistant that can plan, execute, inspect, and learn. It combines many of the latest trends in AI, including multimodality, language and code, and ReAct agents. The paper also includes a cool demo and discusses the latency of the system.</p> </li> <li> <p>GPTeam: Building Human-like Social Behavior in Language Models</p> <p><code>Multi-agent simulation</code> <code>Human-like social behavior</code> <code>Language models</code> <code>Generative agents</code></p> <p>GPTeam is a completely customizable open-source multi-agent simulation, inspired by Stanford\u2019s ground-breaking \u201cGenerative Agents\u201d paper. Every agent within a GPTeam simulation has their own unique personality, memories, and directives, leading to interesting emergent behavior as they interact.</p> </li> <li> <p>LLM Powered Autonomous Agents</p> <p><code>Large Language Model</code> <code>Planning</code> <code>Memory</code> <code>Tool Use</code></p> <p>The article provides a comprehensive overview of building Large Language Model powered agents, including relevant papers, practical applications, and case studies.</p> </li> <li> <p>Best write-up ever on LLM Agents</p> <p><code>LLM</code> <code>NLP</code> <code>OpenAI</code></p> <p>The article provides a comprehensive overview of LLM agents, including their capabilities, limitations, and potential applications. It also discusses the challenges involved in developing and deploying LLM agents, and the ethical considerations that need to be taken into account.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"References <ul> <li> <p>RAG &amp; Enterprise: A Match Made in Heaven</p> <p><code>RAG</code> <code>LLM</code> <code>Enterprise Search</code> <code>Information Retrieval</code></p> <p>RAG (Retrieve and Generate) models are a powerful tool for enterprise search, as they offer flexibility, practicality, broader coverage, and interpretability. Additionally, with the help of tools like LangChain and Google Vertex, it is now easier than ever to implement RAG solutions.</p> </li> <li> <p>HNSW-FINGER: Approximate Nearest Neighbor Search with Locality-Sensitive Hashing</p> <p><code>locality-sensitive hashing</code> <code>approximate nearest neighbor search</code> <code>HNSW</code></p> <p>HNSW-FINGER is a new approximate nearest neighbor search algorithm that uses locality-sensitive hashing to project the query and candidate nodes onto a center node. This allows HNSW-FINGER to achieve better accuracy and efficiency than existing approximate nearest neighbor search algorithms.</p> </li> <li> <p>Vector Databases and Hierarchical Navigable Small World</p> <p><code>Vector Databases</code> <code>Machine Learning</code> <code>Artificial Intelligence</code> <code>Data Science</code> <code>Generative AI</code></p> <p>The article discusses the rise of vector databases in the era of generative AI and introduces Hierarchical Navigable Small World (HNSW) as an efficient indexing method. HNSW builds multiple graph layers with varying densities to optimize the search process and reduce the number of iterations required to find approximate nearest neighbors.</p> </li> <li> <p>RAG-Fusion: A New Retrieval Technique for LLM</p> <p><code>LLM</code> <code>Retrieval</code> <code>MultiQueryRetrieval</code> <code>Reciprocal Rank Fusion</code></p> <p>RAG-Fusion is a new retrieval technique that builds upon the idea of MultiQueryRetrieval. It generates multiple sub queries based on a user question, retrieves documents for each sub query, and merges the retrieved documents together using Reciprocal Rank Fusion.</p> </li> <li> <p>Question Answering over Documents with Retrieval-Augmented Generation</p> <p><code>rag</code> <code>question answering</code> <code>information retrieval</code> <code>llm</code></p> <p>This article describes how to build a question-answering over documents application using LLMs. The article covers the use of retrieval-augmented generation (RAG) for this task, and provides a walkthrough of how to build such an application.</p> </li> <li> <p>Reordering retrieved documents to improve performance</p> <p><code>long context</code> <code>performance degradation</code> <code>retrieval</code></p> <p>When models must access relevant information in the middle of long contexts, they tend to ignore the provided documents. This issue can be avoided by reordering documents after retrieval to avoid performance degradation.</p> </li> <li> <p>How to improve the performance of your LLM search engine with Retrieve &amp; Re-Rank</p> <p><code>LLM</code> <code>Semantic Search</code> <code>Information Retrieval</code> <code>Question Answering</code></p> <p>This article explains how to improve the performance of your LLM search engine with Retrieve &amp; Re-Rank. It covers lexical search, dense retrieval, semantic search and cross-encoders.</p> </li> <li> <p>EAR: Improving Passage Retrieval for Open-Domain Question Answering with Query Expansion and Reranking</p> <p><code>passage retrieval</code> <code>query expansion</code> <code>query reranking</code> <code>open-domain question answering</code></p> <p>EAR is a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to better retrieval results.</p> </li> <li> <p>MS MARCO: A Large Scale Information Retrieval Corpus</p> <p><code>information retrieval</code> <code>semantic search</code> <code>TREC-DL 2019</code> <code>MS Marco Passage Retrieval</code> <code>BM25</code> <code>ElasticSearch</code> <code>electra-base-model</code> <code>cross-encoder</code></p> <p>MS MARCO is a large scale information retrieval corpus that was created based on real user search queries using Bing search engine. It can be used for semantic search, i.e., given keywords / a search phrase / a question, the model will find passages that are relevant for the search query. Performance is evaluated on TREC-DL 2019 and MS Marco Passage Retrieval dataset. As baseline we show the results for lexical search with BM25 using ElasticSearch.</p> </li> <li> <p>Self-querying retriever: A new way to search for information</p> <p><code>LLM</code> <code>VectorStore</code> <code>information retrieval</code></p> <p>A self-querying retriever is a new way to search for information that uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.</p> </li> <li> <p>GenQ: Training Effective Dense Retrieval Models with Synthetic Queries</p> <p><code>dense retrieval</code> <code>bi-encoders</code> <code>sentence transformers</code> <code>text generation</code> <code>synthetic data</code> <code>asymmetric semantic search</code> <code>query generation</code> <code>T5</code> <code>MNR loss</code> <code>Pinecone</code></p> <p>GenQ is a method for training effective dense retrieval models using synthetic queries. It uses a text generation model to generate queries for unlabeled passages of text, which are then used to fine-tune a bi-encoder model. GenQ can achieve performances approaching models trained with supervised methods, and it is particularly useful when we have limited labeled data.</p> </li> <li> <p>InPars-v2: Efficient Dataset Generation for Information Retrieval with Open-Source Language Models</p> <p><code>information retrieval</code> <code>large language models</code> <code>dataset generation</code> <code>open-source</code></p> <p>InPars-v2 is a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. It achieves new state-of-the-art results on the BEIR benchmark.</p> </li> <li> <p>Qdrant: A Vector Database &amp; Vector Similarity Search Engine</p> <p><code>Vector database</code> <code>Vector similarity search</code> <code>Approximate nearest neighbor search</code> <code>Machine learning</code> <code>Artificial intelligence</code></p> <p>Qdrant is a vector database and vector similarity search engine that can be used for building applications such as matching, searching, and recommending. It is easy to use and provides a variety of features such as support for additional payload associated with vectors, payload filtering conditions, and dynamic query planning.</p> </li> <li> <p>AutoMergingRetriever: A New Algorithm for Better Retrieval and RAG</p> <p><code>LLM</code> <code>Retrieval</code> <code>RAG</code> <code>ChatGPT</code> <code>Dynamic Retrieval</code> <code>Semantic Relatedness</code></p> <p>The AutoMergingRetriever algorithm dynamically retrieves less disparate / larger contiguous blobs of context only when you need it. This helps the LLM synthesize better results, but avoids always cramming in as much context as you can.</p> </li> <li> <p>Optimizing RAG With LLMS: Exploring Chunking Techniques and Reranking for Enhanced Results</p> <p><code>LLM</code> <code>Chunking</code> <code>Ranking</code> <code>Retrieval Augmented Generation (RAG)</code></p> <p>This article explores chunking techniques and reranking for enhanced results in the context of optimizing RAG with LLMs. The key points covered include strategies for optimizing RAG, using chunking techniques to streamline processing, and implementing ranking models to enhance search quality.</p> </li> <li> <p>Dynamic chunk length in AutoMergingRetriever</p> <p><code>language-models</code> <code>retrieval</code> <code>summarization</code></p> <p>The AutoMergingRetriever dynamically chooses the chunk length when retrieving information, resulting in better semantic meaning and context.</p> </li> <li> <p>Multi-Document Agents for Building LLM-Powered QA Systems</p> <p><code>RAG</code> <code>LLM</code> <code>QA</code> <code>summarization</code> <code>multi-document agents</code></p> <p>The article introduces a new approach for building LLM-powered QA systems that can scale to large numbers of documents and question types. The approach uses multi-document agents, which are able to answer a broad set of questions, including fact-based QA over single documents, summarization over single documents, fact-based comparisons over multiple documents, and holistic comparisons across multiple documents.</p> </li> <li> <p>How to Improve Your RAG App: Adjusting Chunk Size</p> <p><code>RAG</code> <code>chunk size</code> <code>retrieval</code> <code>ranking</code> <code>evaluation</code></p> <p>Adjusting chunk size is an important step in improving the performance of a RAG app. More chunks do not always lead to better results, and reranking retrieved chunks may not necessarily improve results either. To find the optimal chunk size, it is necessary to define an evaluation benchmark and perform a sweep over chunk sizes and top-k values. The Arize AI team has provided a comprehensive Colab notebook and slides that demonstrate how to run chunk size sweeps and perform retrieval and Q&amp;A evaluations with Phoenix and LlamaIndex.</p> </li> <li> <p>How to Choose the Right Chunk Size for Your RAG System</p> <p><code>RAG system</code> <code>chunk size</code> <code>response time</code> <code>faithfulness</code> <code>relevancy</code></p> <p>Choosing the right chunk size for a RAG system is critical for efficiency and accuracy. The optimal chunk size strikes a balance between capturing essential information and speed. The article provides a practical evaluation setup to determine the right chunk size for a specific use case and dataset.</p> </li> <li> <p>RAG-Fusion: The Next Frontier of Search Technology</p> <p><code>Reciprocal Rank Fusion</code> <code>Query Generation</code> <code>Retrieval Augmented Generation</code> <code>Vector Search</code></p> <p>RAG-Fusion is a search methodology that aims to bridge the gap between traditional search paradigms and the multifaceted dimensions of human queries. It employs multiple query generation and Reciprocal Rank Fusion to re-rank search results, with the goal of unearthing transformative knowledge that often remains hidden behind top search results.</p> </li> <li> <p>RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation</p> <p><code>language-models</code> <code>retrieval-augmentation</code> <code>compression</code> <code>abstractive-summarization</code> <code>extractive-summarization</code></p> <p>We propose a method to improve the performance of retrieval-augmented language models (LMs) by compressing the retrieved documents into textual summaries. Our method, RECOMP, achieves a compression rate of as low as 6% with minimal loss in performance for both language modeling and open domain question answering tasks. We also show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide summaries largely faithful to the retrieved documents.</p> </li> <li> <p>Optimizing Retrieval and Generation Performance in Large Language Models</p> <p><code>RAG</code> <code>Machine Learning</code> <code>Knowledge Retrieval</code> <code>AI</code></p> <p>This article discusses various techniques for optimizing retrieval and generation performance in large language models, including decoupling chunks for retrieval and synthesis, using structured retrieval techniques, dynamically retrieving chunks based on tasks, and optimizing context embeddings.</p> </li> <li> <p>Scaling Retrieval-Augmented LLM to 48B</p> <p><code>LLM Scaling</code> <code>Retrieval-Augmented LLM</code> <code>Instruction Tuning</code></p> <p>NVIDIA introduces Retro 48B, the largest LLM pretrained with retrieval. It shows significant perplexity improvement over GPT 43B and can be instruction-tuned more effectively, achieving +7% improvement on zero-shot question-answering tasks.</p> </li> <li> <p>Parsing complex documents with embedded tables using unstructured.io and LlamaIndex</p> <p><code>unstructured.io</code> <code>LlamaIndex</code> <code>SEC filings</code> <code>research papers</code> <code>invoices</code></p> <p>Parsing complex documents with embedded tables can be done using unstructured.io and LlamaIndex. This is especially relevant for SEC filings, research papers, invoices, and more.</p> </li> <li> <p>LLM Production Ready RAGs</p> <p><code>LLM</code> <code>RAG</code> <code>Best Practices</code></p> <p>This talk will discuss best practices for creating production ready RAGs in the context of LLMs.</p> </li> <li> <p>Joint Tabular/Semantic QA over Tesla 10K</p> <p><code>LLM</code> <code>NLP</code> <code>Information Retrieval</code> <code>Question Answering</code></p> <p>This article demonstrates how to ask questions over Tesla's 10K report with understanding of both the unstructured text as well as embedded tables. It utilizes Unstructured to parse out the tables and LlamaIndex recursive retrieval to index and retrieve tables if necessary given the user question.</p> </li> <li> <p>New Fine-Tuning Features in LlamaIndex</p> <p><code>fine-tuning</code> <code>retrieval augmentation</code> <code>structured outputs</code></p> <p>This week, LlamaIndex added a lot of new fine-tuning features, including fine-tuning with retrieval augmentation and fine-tuning for better structured outputs.</p> </li> <li> <p>SuperKnowa: Building Reliable RAG Pipelines for Enterprise LLM Applications</p> <p><code>RAG</code> <code>LLM</code> <code>NLP</code> <code>Generative AI</code> <code>Enterprise AI</code></p> <p>This article introduces SuperKnowa, a framework for building reliable and scalable RAG pipelines for enterprise LLM applications. It discusses the challenges of taking a RAG PoC to production and how SuperKnowa addresses these challenges. The article also provides an overview of the SuperKnowa framework and its features, including data indexing, context-aware queries, model evaluation, and debugging.</p> </li> <li> <p>SEC Insights: A real-world full-stack application using LlamaIndex</p> <p><code>LLM</code> <code>RAG</code> <code>SEC Insights</code> <code>Tutorial</code> <code>Open Source</code></p> <p>This repository contains the code for SEC Insights, a real-world full-stack application that uses the Retrieval Augmented Generation (RAG) capabilities of LlamaIndex to answer questions about SEC 10-K &amp; 10-Q documents. The application is open source and available on GitHub. A tutorial video is also available on YouTube.</p> </li> <li> <p>Text Ranking with Pretrained Transformers</p> <p><code>Text Ranking</code> <code>Transformers</code> <code>BERT</code> <code>Self-supervised Learning</code> <code>Natural Language Processing</code> <code>Information Retrieval</code></p> <p>This survey provides an overview of text ranking with neural network architectures known as transformers. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage architectures and dense retrieval techniques that perform ranking directly.</p> </li> <li> <p>8 Key Considerations for Building Production-Grade LLM Apps</p> <p><code>LLM</code> <code>RAG</code> <code>Embeddings</code> <code>Data Pipelines</code> <code>Scalability</code> <code>Retrieval</code> <code>Entity Lookup</code></p> <p>This article discusses 8 key considerations for building production-grade LLM apps over your data. These considerations include using different chunks for retrieval and synthesis, using embeddings that live in a different latent space than the raw text, dynamically loading/updating the data, designing the pipeline for scalability, storing data in a hierarchical fashion, using robust data pipelines, and using hybrid search for entity lookup.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#embeddings-for-retrieval","title":"Embeddings for Retrieval","text":"References <ul> <li> <p>How to use Aleph Alpha's semantic embeddings</p> <p><code>embeddings</code> <code>semantic embeddings</code> <code>Aleph Alpha</code></p> <p>There are two ways to use Aleph Alpha's semantic embeddings: asymmetric embeddings and symmetric embeddings.</p> </li> <li> <p>TaylorAI/gte-tiny: A 45MB Tiny Model That Beats Existing Sentence-Transformer Embeddings</p> <p><code>Vector Search</code> <code>Sentence Transformer</code> <code>Embedding</code> <code>VectorDB</code> <code>MTEB Leaderboard</code></p> <p>The paper introduces TaylorAI/gte-tiny, a 45MB tiny model that beats existing sentence-transformer embedders. The model is based on BERT and distilled from thenlper/gte-small. It achieves comparable performance to larger models while being much smaller and faster. The model ranks 28th out of 126 models on the MTEB leaderboard.</p> </li> <li> <p>LLM-based Sentence Embeddings</p> <p><code>LLM</code> <code>Sentence Embeddings</code> <code>HuggingFaceEmbeddings</code> <code>SentenceTransformerEmbeddings</code> <code>Sentence-BERT</code></p> <p>This article introduces a new way to generate sentence embeddings using LLM. The method is based on the HuggingFaceEmbeddings integration, which allows users to use SentenceTransformers embeddings directly. The article also provides an example of how to use the new method.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#evaluating-rags","title":"Evaluating RAGs","text":"References <ul> <li> <p>Ragas: A Framework for Evaluating Retrieval Augmented Generation Pipelines</p> <p><code>LLM</code> <code>RAG</code> <code>NLP</code> <code>Machine Learning</code></p> <p>Ragas is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines. It provides you with the tools based on the latest research for evaluating LLM-generated text to give you insights about your RAG pipeline. Ragas can be integrated with your CI/CD to provide continuous checks to ensure performance.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#integrating-llms-with-knowledge-graphs","title":"Integrating LLMs with Knowledge Graphs","text":"References <ul> <li> <p>LLMs and Knowledge Graphs</p> <p><code>Knowledge Graphs</code> <code>LLMs</code> <code>RAG</code> <code>Vector Databases</code> <code>ChromaDB</code></p> <p>This article discusses the advantages and disadvantages of using Knowledge Graphs (KGs) with LLMs. It also provides a link to a Colab notebook and a video tutorial on the topic.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#llm-watermarking","title":"LLM Watermarking","text":"References <ul> <li> <p>AI generated text? New research shows watermark removal is harder than one thinks!</p> <p><code>LLM</code> <code>Watermarking</code> <code>Text Generation</code> <code>AI Ethics</code></p> <p>Researchers from the University of Maryland have found that it is much harder to remove watermarks from AI-generated text than previously thought. This has implications for the use of watermarks to detect machine-generated content, such as spam and harmful content.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#llm-application-development","title":"LLM Application Development","text":""},{"location":"session_1/part_3_landscape_of_llms/#llm-saas-apps","title":"LLM SaaS Apps","text":"References <ul> <li> <p>Introducing LLM Studio: A Powerful Platform for Building and Deploying Language Models</p> <p><code>LLM</code> <code>NLP</code> <code>Machine Learning</code></p> <p>LLM Studio is a powerful platform that enables developers to easily build, train, and deploy language models. With its user-friendly interface and comprehensive set of features, LLM Studio makes it easy to create and deploy state-of-the-art language models for a variety of applications.</p> </li> <li> <p>Verba: The Open-Source LLM-Based Search Engine</p> <p><code>LLM</code> <code>Open Source</code> <code>Search Engine</code></p> <p>Verba is an open-source LLM-based search engine that supports a broad spectrum of open-source libraries and custom features. It is easy to install and use, and it does not require users to give away any of their data.</p> </li> </ul>"},{"location":"session_1/part_3_landscape_of_llms/#llm-courses","title":"LLM Courses","text":"References <ul> <li> <p>LLM course recommendations</p> <p><code>LLM</code> <code>NLP</code> <code>AI</code></p> <p>The article recommends some short courses on LLM. The author also recommends some YouTube channels and videos on LLM.</p> </li> </ul>"},{"location":"session_4/","title":"Session 4 - Training and Evaluating LLMs On Custom Datasets","text":"<p>This session aims to equip you with the knowledge to train Large Language Models (LLMs) by exploring techniques like unsupervised pretraining and supervised fine-tuning with various preference optimization methods. It will also cover efficient fine-tuning techniques, retrieval-based approaches, and language agent fine-tuning. Additionally, the session will discuss LLM training frameworks and delve into evaluation methods for LLMs, including evaluation-driven development and using LLMs for evaluation itself.</p> <p>This session is aimed to help:</p> <ul> <li>People who are already familiar basics of LLMs and Transformers</li> <li>People who already knows how to use pre-trained LLMs prompt engineering and RAG</li> <li>People who want train or finetune their own LLMs on custom data.</li> <li>People who want to lear how to evaluate LLMs</li> </ul>"},{"location":"session_4/#outline","title":"Outline","text":""},{"location":"session_4/#part-1-training-foundational-llms","title":"Part 1: Training Foundational LLMs","text":"<p>Coming soon...</p>"},{"location":"session_4/#part-2-finetuning-lms-to-human-preferences","title":"Part 2: Finetuning LMs To Human Preferences","text":""},{"location":"session_4/#details","title":"Details","text":"<ul> <li>Date: 14 March, 2024</li> <li>Speaker: Abhor Gupta</li> <li>Location: Infocusp Innovations LLP</li> </ul>"},{"location":"session_4/#material","title":"Material","text":"<ul> <li>Recording: TODO</li> </ul>"},{"location":"session_4/#part-3-llm-training-frameworks","title":"Part 3: LLM Training Frameworks","text":"<p>Coming soon...</p>"},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/","title":"RLHF","text":"<p>Reinforcement learning from human feedback (RLHF) is a transformative technique that enables us to fine-tune large language models (LLMs) or transformer-based models for improved alignment with our intended goals. This approach goes beyond the standard techniques that train LLMs on massive volumes of text data. RLHF uses human feedback to teach LLMs how to better adhere to our preferences and values.</p> <p>There are several very well written blogs on the topic - here, here and here. I am especially fond of the one written by Chip Huyen here. The intention behind writing this is to understand RLHF using a simple and mostly self-contained implementation to solve a demonstrative problem. Let it be sufficiently trivial that we may open up our model and visually observe some effects of RLHF using different techniques.</p> <p>Let us first go over some basics.</p> <p>Can't help but love this beautiful depiction of RLHF among the broader spectrum of an LLM's training, by twitter.com/anthrupad.</p> <p></p> <p>The image above shows the different methods of training an LLM, their \"size\" in terms of the space of outputs they represent and their \"quality\" in terms of their social acceptance to humans:</p> <ol> <li>Unsupervised Learning: Train an LLM on a massive corpus of raw text; this teaches the LLM a language - the fundamentals of its structure, grammer and relationship between words. In terms of its objective, the LLM is trained to predict the next word in a context. But! Though an LLM may know a language, it doesn't necessarily know how to converse. In its space of outputs, it is aware of what it can do, but not necessarily what it should do. It is like the Shoggoth, massive but ugly.</li> <li>Supervised Fine-tuning: Tailor the LLM to specific tasks like translation, question-answering, or summarization. Here, the LLM is trained on a set of input-output pairs demonstrating the task. Following the example from the point above, this is akin to teaching the LLM how to converse. Its output space here is refined to answer in specific ways, perhaps with domain specific know-how, or in accordance to a particular task. This is like the deformed human face, you'll accept it but it's not necessarily very pleasing.</li> <li>RLHF: Refine the LLM's output to better align with human values, preferences, and safety guidelines. The training here involves giving feedback signals on the LLM's output to guide it to some desired behavior. Following the same context from (1) and (2), after it has learnt language and knows how to converse, it learns to adhere to the social norms. Within its output space, this is the refinement that narrows down the conversation ability of the LLM to answer in a way that pleases its general reader - ethical speech, truthful statements, intelligent use of vocabulary etc. It is the smiley face that you want to talk to. :)</li> </ol> <p>For the scope of this notebook, we will only be exploring RLHF. Supervised training will be a part - though it is more a requirement for the sake of thoroughness, than an intented guide on the topic. Therefore, I'll be using a very simple supervised pretraining that is closer to the description of supervised fine-tuning above, than unsupervised learning.</p> <p>A complete RLHF pipeline requires the following components:</p> <ol> <li>A pre-trained base model: We begin with a pre-trained LLM. This is a powerful language model that has already learned the intricacies of language structure from vast amounts of text data. This may be followed by supervised fine-tuning to attune the LLM to a specific task like question-answering or summarization.</li> <li>Training a reward model from human feedback: We then create a \"reward model\" specifically designed to capture human preferences. This involves gathering human feedback on various LLM outputs, where people rate the responses based on their desired qualities like helpfulness, safety, and adherence to instructions. By analyzing this feedback, the reward model learns to assign scores to future LLM responses, essentially mimicking human judgment.</li> <li>Fine tuning using Reinforcement Learning: Finally, we put the LLM and the reward model to work together. When presented with a prompt, the LLM generates multiple potential responses. Each response is then analyzed by the reward model, which assigns a score based on its learned understanding of human preferences. Finally, a reinforcement learning algorithm like PPO uses these scores to fine-tune the LLM's internal parameters. Responses that received higher scores become more likely to be generated in the future, while those with lower scores are gradually downplayed. This iterative process progressively aligns the LLM's outputs with human expectations and values.</li> </ol> <p>This pipeline effectively utilizes human feedback to bridge the gap between raw LLM capabilities and human-desired outcomes. It allows us to shape these powerful language models into not just masters of language, but also responsible and valuable tools aligned with our needs.</p> <p>The most prevelant example of RLHF being applied in AI is for text generation to align chatbots with human preferences (InstructGPT, ChatGPT, Gemini are famous examples). Similarly, RLHF has seen application in image generation as well (ImageReward, DPOK). Though limited, some research groups have also explored its application in games  (DeepMind and OpenAI).</p> <p>Even though currently the applications of RLHF in AI are limited, the scope for RLHF is much wider.</p> <p>Do you use e-commerce websites like Amazon? Do you use Uber for requesting cabs? Do you use Google Maps for deciding which restaurant, bar or hospital to visit? You must have seen ratings for products, or people, or services, or food. You likely would have given some yourself. These are all examples of human feedback. And when these affect the product or service to comform to user satisfaction, it is also a form of RLHF.</p> <p>Take, for instance, cooking robots are a thing now (Moley, Nymble). For the food that is cooked by the robots based on some recipe, the recipe can be adjusted for duration of cooking, quantity of spices etc for user preference based on their feedback. Self-driving cars are also real now (Waymo, Tesla). Based on customer's feedback, the ride be adjusted to be faster/slower, less jerky, smoother maneuverability.</p> <p>In the next section, we will establish a small toy problem to solve using a tiny LLM. Then we will dive into each of the RLHF concepts in detail along with some code to establish an implementational understanding as well some nice visualizations to complement our findings.</p> <p>To keep the scale of things simple, let us work with a \"language\" of numbers. Our vocabulary consist of digits 0-9 as well as a special digit 10 that separates our input and output.</p> <p>Typically for large LLMs, the language training is followed by a task specialisation training like question-answering before moving on to RLHF. To keep things simple, we avoid differentiating between language training and task specialisation and do a supervised training once to get our base model.</p> <p>The structure of the language is that for the current output to be generated, one of the last four digits (in the whole sequence of input+output) is chosen and its increment modulo 10 is outputted.</p> <p>Given $a_1, a_2, ..., a_{n-1}, a_n$, $\\forall n &gt; 4$, $$a'_{n+1} \\sim \\{a_{n-3}, a_{n-2}, a_{n-1}, a_{n}\\}$$ $$a_{n+1} = (a'_{n+1} + 1)\\ \\%\\ 10$$</p> <p>For example, Input: 4, 5, 9, 1</p> <p>Generation: 4, 5, 9, 1, 10, 6 4, 5, 9, 1, 10, 6, 2 4, 5, 9, 1, 10, 6, 2, 7 4, 5, 9, 1, 10, 6, 2, 7, 8</p> <p>The underlined digits are the sampling set for the output. The digit marked in red is the digit sampled. Finally, the digit bolded at the end of the sequence is generated as the increment of the sampled digit.</p> <p>Now we need a \"preference\" to incorporate. Mathematically, we wish to adjust the probability of some generation rule learned during the previous step.</p> <p>For this toy example, let's change only the generation of the first output token to be the increment of the last input token.</p> <p>That is, given $a_1, a_2, ..., a_{n-1}, a_n$, $\\forall n &gt; 4$, $$     a_{n+1}= \\begin{cases}     a_n + 1,&amp; \\text{if } {a_{n+1}} = y_1\\\\     (a'_{n+1} + 1)\\ \\%\\ 10, &amp; \\text{otherwise} \\end{cases} $$</p> <p>Where $y_1$ is the first output token.</p> <p>Considering the same example as above, Input: 4, 5, 9, 1</p> <p>Generation: 4, 5, 9, 1, 10, 2 (notice the change in underline) 4, 5, 9, 1, 10, 2, 2 4, 5, 9, 1, 10, 2, 2, 3 4, 5, 9, 1, 10, 2, 2, 3, 4</p> <p>For the first generation step, the model will only sample from the last digit and continue the original rule thereafter. Notice that except for the first generation step, the rest of the outputs are sampled from the same positions as the earlier example, yet the entire sequence has changed. Thus, though we wish to preserve the rule for the rest of the outputs, the actual outputs are not independent of the first output. Therefore, this small change in the output of a single token can have a cascading effect and lead to very different generations.</p> <p>This is in line with the spirit of RLHF, where say, if we want to reduce the toxicity, reducing the probability of toxic words will have a cascading effect and we do not need to affect the probability of several thousands of unrelated words.</p> <p>Before RLHF: Enough talking, show me the code! After RLHF: You've explained what we're doing here well enough. We would like to move on to the implementational details.</p> <p>\ud83d\ude09</p> <p>First we learn the language using supervised learning. I'm using Karpathy's minGPT for the LLM and supervised training.</p> <p></p> <p>Source: HuggingFace - RLHF blog</p> <p>Imports --</p> <p>NOTE:  You can find this notebook and related code here.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom mingpt.utils import set_seed\nimport numpy as np\nset_seed(3407)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n</pre> import torch from torch.utils.data import Dataset from torch.utils.data.dataloader import DataLoader from mingpt.utils import set_seed import numpy as np set_seed(3407)  device = 'cuda' if torch.cuda.is_available() else 'cpu' <p>Hyperparams for size of vocab and length of input --</p> In\u00a0[\u00a0]: Copied! <pre>VOCAB_SIZE = 10\nINPUT_SIZE = 4\n</pre> VOCAB_SIZE = 10 INPUT_SIZE = 4 <p>Class for generating training pairs for supervised language learning --</p> In\u00a0[\u00a0]: Copied! <pre>class SupervisedDataset(Dataset):\n    \"\"\"\n    Problem: Look at last 4 digits and sample one of them to output its increment.\n\n    Input: 3 8 1 4\n    Possible ouputs: 2 2 3 5 || 5 9 6 0 || 2 9 5 3 etc\n\n    Which will feed into the transformer concatenated as:\n    input:  3 8 1 4 S 2 2 3\n    output: I I I I 2 2 3 5\n    where I is \"ignore\", and S is the separation token\n    \"\"\"\n\n    def __init__(self):\n        self.EOS = VOCAB_SIZE\n\n    def __len__(self):\n        return 10000 # ...\n\n    def get_vocab_size(self):\n        return VOCAB_SIZE+1 # normal vocab + serparation token\n\n    def __getitem__(self, idx):\n        inputs = torch.randint(VOCAB_SIZE, size=(INPUT_SIZE,), dtype=torch.long)\n        ouptputs = []\n\n        # Create input output pairs\n        inp = np.random.randint(VOCAB_SIZE, size=(INPUT_SIZE,)).tolist()\n        sol = []\n        for i in range(INPUT_SIZE):\n            sol.append((np.random.choice(inp[i:] + sol) + 1)%10)\n\n        # concatenate the problem specification and the solution\n        cat = torch.Tensor(inp + [self.EOS] + sol).long()\n\n        # the inputs to the transformer will be the offset sequence\n        x = cat[:-1].clone()\n        y = cat[1:].clone()\n\n        # we only want to predict at output locations, mask out the loss at the input locations\n        y[:INPUT_SIZE] = -1\n        return x, y\n</pre> class SupervisedDataset(Dataset):     \"\"\"     Problem: Look at last 4 digits and sample one of them to output its increment.      Input: 3 8 1 4     Possible ouputs: 2 2 3 5 || 5 9 6 0 || 2 9 5 3 etc      Which will feed into the transformer concatenated as:     input:  3 8 1 4 S 2 2 3     output: I I I I 2 2 3 5     where I is \"ignore\", and S is the separation token     \"\"\"      def __init__(self):         self.EOS = VOCAB_SIZE      def __len__(self):         return 10000 # ...      def get_vocab_size(self):         return VOCAB_SIZE+1 # normal vocab + serparation token      def __getitem__(self, idx):         inputs = torch.randint(VOCAB_SIZE, size=(INPUT_SIZE,), dtype=torch.long)         ouptputs = []          # Create input output pairs         inp = np.random.randint(VOCAB_SIZE, size=(INPUT_SIZE,)).tolist()         sol = []         for i in range(INPUT_SIZE):             sol.append((np.random.choice(inp[i:] + sol) + 1)%10)          # concatenate the problem specification and the solution         cat = torch.Tensor(inp + [self.EOS] + sol).long()          # the inputs to the transformer will be the offset sequence         x = cat[:-1].clone()         y = cat[1:].clone()          # we only want to predict at output locations, mask out the loss at the input locations         y[:INPUT_SIZE] = -1         return x, y <p>Looking at one sample --</p> In\u00a0[\u00a0]: Copied! <pre># Supervised dataset\nst_dataset = SupervisedDataset()\nst_dataset[0]\n</pre> # Supervised dataset st_dataset = SupervisedDataset() st_dataset[0] Out[\u00a0]: <pre>(tensor([ 4,  3,  7,  6, 10,  8,  4,  8]),\n tensor([-1, -1, -1, -1,  8,  4,  8,  5]))</pre> <p>Create a GPT instance --</p> In\u00a0[\u00a0]: Copied! <pre>from mingpt.model import GPT\n\ndef get_model(block_size, vocab_size, output_size=None):\n    '''\n    block_size = length of input\n    vocab_size = digits allowed\n    output_size = length of output\n    '''\n    if output_size is None:\n        output_size = vocab_size\n    model_config = GPT.get_default_config()\n    model_config.model_type = 'gpt-nano'\n    model_config.vocab_size = vocab_size\n    model_config.block_size = block_size\n    model_config.output_size = output_size\n    model = GPT(model_config)\n    return model\n\nst_model = get_model(INPUT_SIZE*2, st_dataset.get_vocab_size())\n</pre> from mingpt.model import GPT  def get_model(block_size, vocab_size, output_size=None):     '''     block_size = length of input     vocab_size = digits allowed     output_size = length of output     '''     if output_size is None:         output_size = vocab_size     model_config = GPT.get_default_config()     model_config.model_type = 'gpt-nano'     model_config.vocab_size = vocab_size     model_config.block_size = block_size     model_config.output_size = output_size     model = GPT(model_config)     return model  st_model = get_model(INPUT_SIZE*2, st_dataset.get_vocab_size()) <p>Set up training --</p> In\u00a0[\u00a0]: Copied! <pre># create a Trainer object\nfrom mingpt.trainer import Trainer\n\ntrain_config = Trainer.get_default_config()\ntrain_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\ntrain_config.max_iters = 5000\ntrain_config.num_workers = 0\ntrainer = Trainer(train_config, st_model, st_dataset)\n</pre> # create a Trainer object from mingpt.trainer import Trainer  train_config = Trainer.get_default_config() train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster train_config.max_iters = 5000 train_config.num_workers = 0 trainer = Trainer(train_config, st_model, st_dataset) <p>Training --</p> In\u00a0[\u00a0]: Copied! <pre>def batch_end_callback(trainer):\n    if trainer.iter_num % 100 == 0:\n        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\ntrainer.set_callback('on_batch_end', batch_end_callback)\n\ntrainer.run()\n</pre> def batch_end_callback(trainer):     if trainer.iter_num % 100 == 0:         print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\") trainer.set_callback('on_batch_end', batch_end_callback)  trainer.run() <p>Loss stabilizes around 1.25. It cannot go lower because we don't have fixed outputs. We are trying to have a probability distribution such that multiple outputs have equal probability of being sampled.</p> <p>Now let us give the model a random input and see what the model has learned to generate as the next token --</p> In\u00a0[\u00a0]: Copied! <pre>x, _ = st_dataset[0]\nx = x[:INPUT_SIZE+1].reshape(1, -1)\nprint(\"Input:\", x)\nprint(\"Possible outputs:\", torch.arange(11)[torch.nn.Softmax(dim=-1)(st_model(x)[0])[0, -1] &gt; 0.1])\n</pre> x, _ = st_dataset[0] x = x[:INPUT_SIZE+1].reshape(1, -1) print(\"Input:\", x) print(\"Possible outputs:\", torch.arange(11)[torch.nn.Softmax(dim=-1)(st_model(x)[0])[0, -1] &gt; 0.1]) <pre>Input: tensor([[ 0,  2,  8,  6, 10]])\nPossible outputs: tensor([1, 3, 7, 9])\n</pre> <p>Works like a charm!</p> <p>Let's save the model too. We'll need to load it later before we start the RL training --</p> In\u00a0[\u00a0]: Copied! <pre># Save model weights\ntorch.save(st_model.state_dict(), \"models/minimal_RLHF_basic_supervised.pt\")\n</pre> # Save model weights torch.save(st_model.state_dict(), \"models/minimal_RLHF_basic_supervised.pt\") <p>Now we will train a reward model.</p> <p>The data required to train the reward model is collected as preferences in the format: &lt;promp&gt;, &lt;accepted_response&gt;, &lt;rejected_response&gt;</p> <p>The accepted and rejected responses are simply two difference generations by the supervised training model with human labels marking their preference among the two.</p> <p></p> <p>Source: HuggingFace - RLHF blog</p> <p>I don't have the money to hire humans to do this labeling and neither the time myself to do it. :) So here's a dataset class that'll generate the required data for us--</p> In\u00a0[\u00a0]: Copied! <pre>class PreferenceDataset(Dataset):\n    \"\"\"\n    Same as MyDataset, except this has output as &lt;x, y', y&gt; where x and y are input output from MyDataset. y' is the output that is sampled\n    from preferred distribution - and is preferred over y.\n    \"\"\"\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def get_vocab_size(self):\n        return self.dataset.get_vocab_size()\n\n    def __getitem__(self, idx):\n        x, y = self.dataset[idx]\n\n        _x = x[:INPUT_SIZE+1]\n        _y_reject = torch.concat([y[-INPUT_SIZE:], torch.Tensor([11]).long()])\n        _y_accept = _y_reject.clone()\n\n        # Replace first element with increment of last digit in input\n        _y_accept[0] = (_x[INPUT_SIZE-1] + 1) % 10\n        if _y_accept[0] == _y_reject[0]:\n            _y_reject[0] = (_y_accept[0] - np.random.randint(1, 10)) % 10\n\n        return _x, _y_accept, _y_reject\n</pre> class PreferenceDataset(Dataset):     \"\"\"     Same as MyDataset, except this has output as  where x and y are input output from MyDataset. y' is the output that is sampled     from preferred distribution - and is preferred over y.     \"\"\"     def __init__(self, dataset):         self.dataset = dataset      def __len__(self):         return len(self.dataset)      def get_vocab_size(self):         return self.dataset.get_vocab_size()      def __getitem__(self, idx):         x, y = self.dataset[idx]          _x = x[:INPUT_SIZE+1]         _y_reject = torch.concat([y[-INPUT_SIZE:], torch.Tensor([11]).long()])         _y_accept = _y_reject.clone()          # Replace first element with increment of last digit in input         _y_accept[0] = (_x[INPUT_SIZE-1] + 1) % 10         if _y_accept[0] == _y_reject[0]:             _y_reject[0] = (_y_accept[0] - np.random.randint(1, 10)) % 10          return _x, _y_accept, _y_reject  <p>Let's look at one datapoint in this dataset --</p> In\u00a0[\u00a0]: Copied! <pre>pf_dataset = PreferenceDataset(st_dataset)\npf_dataset[0]\n</pre> pf_dataset = PreferenceDataset(st_dataset) pf_dataset[0] Out[\u00a0]: <pre>(tensor([ 8,  0,  3,  4, 10]),\n tensor([ 5,  4,  5,  5, 11]),\n tensor([ 1,  4,  5,  5, 11]))</pre> <p>The first tensor is the &lt;input&gt;, the second is the &lt;accepted_reponse&gt; and the last is the &lt;rejected_response&gt;. Notice the &lt;accepted_response&gt; and &lt;rejected_response&gt; only differ in their first digits. Unlike a usual RLHF pipeline where the pretrained model would be used to generate the outputs to be ranked for preference, here we artifically generate the data to look like this for our convenience.</p> <p>Finally, it is time to train the reward model. For this we use the following loss function:</p> <p>$$loss = -log(\\sigma(R_{acc} - R_{rej}))$$</p> <p>Where $\\sigma$ is the sigmoid function, $R_{acc}$ and $R_{rej}$ are the rewards obtained by passing the &lt;accepted_response&gt; and &lt;rejected_response&gt; through the reward model. The intuition behind the loss function is to increase the difference between the rewards of the two types of responses. This becomes clear by looking at the training plot below.</p> In\u00a0[\u00a0]: Copied! <pre>import tqdm\n\n# Hyperparams\nepochs = 40\nbatch_size = 64\nrm_lr = 1e-4\nacc_list = []\nrej_list = []\n\n# Dataloader\ntrain_loader  = DataLoader(pf_dataset, shuffle=False, batch_size=batch_size)\n\n# Optimizer\nreward_model = get_model(block_size=INPUT_SIZE*2+2, vocab_size=pf_dataset.get_vocab_size()+1, output_size=1)\nrm_opt = torch.optim.Adam(reward_model.parameters(), lr=rm_lr)\n\n# Training\nreward_model.train()\nfor ep_i in tqdm.tqdm(range(epochs)):\n    for b_i, batch in enumerate(train_loader):\n        inp, acc, rej = batch\n\n        # Get rewards\n        r_acc = reward_model(torch.concat([inp, acc], dim=-1))[0][:, -1, 0]\n        r_rej = reward_model(torch.concat([inp, rej], dim=-1))[0][:, -1, 0]\n\n        # Loss and backprop\n        loss = -torch.log(torch.nn.Sigmoid()(r_acc-r_rej)).mean()\n        rm_opt.zero_grad()\n        loss.backward()\n        rm_opt.step()\n\n        # Save for plotting\n        acc_list.append(r_acc.mean().detach().item())\n        rej_list.append(r_rej.mean().detach().item())\n\n#     print(ep_i, np.mean(acc_list[-20:]), np.mean(rej_list[-20:]))\n</pre> import tqdm  # Hyperparams epochs = 40 batch_size = 64 rm_lr = 1e-4 acc_list = [] rej_list = []  # Dataloader train_loader  = DataLoader(pf_dataset, shuffle=False, batch_size=batch_size)  # Optimizer reward_model = get_model(block_size=INPUT_SIZE*2+2, vocab_size=pf_dataset.get_vocab_size()+1, output_size=1) rm_opt = torch.optim.Adam(reward_model.parameters(), lr=rm_lr)  # Training reward_model.train() for ep_i in tqdm.tqdm(range(epochs)):     for b_i, batch in enumerate(train_loader):         inp, acc, rej = batch          # Get rewards         r_acc = reward_model(torch.concat([inp, acc], dim=-1))[0][:, -1, 0]         r_rej = reward_model(torch.concat([inp, rej], dim=-1))[0][:, -1, 0]          # Loss and backprop         loss = -torch.log(torch.nn.Sigmoid()(r_acc-r_rej)).mean()         rm_opt.zero_grad()         loss.backward()         rm_opt.step()          # Save for plotting         acc_list.append(r_acc.mean().detach().item())         rej_list.append(r_rej.mean().detach().item())  #     print(ep_i, np.mean(acc_list[-20:]), np.mean(rej_list[-20:])) In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\nimport numpy as np\n\ndef moving_average(a, n=10):\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\nplt.plot(moving_average(acc_list, 20), label=\"R_acc\")\nplt.plot(moving_average(rej_list, 20), label=\"R_rej\")\nplt.legend()\nplt.title(f\"Reward model learning\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Reward (moving average)\")\n</pre> from matplotlib import pyplot as plt import numpy as np  def moving_average(a, n=10):     ret = np.cumsum(a, dtype=float)     ret[n:] = ret[n:] - ret[:-n]     return ret[n - 1:] / n  plt.plot(moving_average(acc_list, 20), label=\"R_acc\") plt.plot(moving_average(rej_list, 20), label=\"R_rej\") plt.legend() plt.title(f\"Reward model learning\") plt.xlabel(\"Epochs\") plt.ylabel(\"Reward (moving average)\") Out[\u00a0]: <pre>Text(0, 0.5, 'Reward (moving average)')</pre> <p>Let's take a look at the kind of rewards the model generates for a fixed sequence and different values at the first position of the output.</p> In\u00a0[\u00a0]: Copied! <pre>i = 0\nfor i in range(11):\n    print(i, \"||\", reward_model(torch.Tensor([[ 6,  1,  7,  0, 10,  i,  5,  5,  6, 11]]).long())[0][0, -1, 0].item())\n</pre> i = 0 for i in range(11):     print(i, \"||\", reward_model(torch.Tensor([[ 6,  1,  7,  0, 10,  i,  5,  5,  6, 11]]).long())[0][0, -1, 0].item()) <pre>0 || -2.0374112129211426\n1 || 4.983941078186035\n2 || -0.8424915075302124\n3 || -6.84158182144165\n4 || -6.4326276779174805\n5 || -6.820979118347168\n6 || -6.832927703857422\n7 || -6.83734130859375\n8 || -6.613164901733398\n9 || -4.2613844871521\n10 || -6.424010753631592\n</pre> <p>Given the input sequence [6, 1, 7, 0], the first output token should be last_token+1 = 0 + 1 = 1. All other generations are \"wrong\", so the reward model gives positive reward for token 1 and negative rewards for others.</p> <p>Finally, we have arrived at our final training that will use our reward model to update the supervised learning model using reinforecement learning.</p> <p></p> <p>Source: HuggingFace - RLHF blog</p> <p>Following is a function for calculating logprob given a model and outputs. This'll help us calculate loss for RL and KL Divergence. More details on these a few code blocks below --</p> In\u00a0[\u00a0]: Copied! <pre>from torch.distributions import Categorical\n\ndef get_logprob(agent, outputs):\n    '''\n    Get the logprobs for outputs acc. to agent's policy.\n\n    Args:\n        agent: Actor network (or reference)\n        outputs: output ids\n            Shape = (sequence, tokens)\n\n    returns\n        logprob of outputs acc to agent's policy\n            Shape = (sequence, tokens)\n    '''\n    logits = agent(outputs[:, :-1])[0][:, -INPUT_SIZE:, :]\n    logprob = Categorical(logits=logits).log_prob(outputs[:, -INPUT_SIZE:])\n    return logprob\n</pre> from torch.distributions import Categorical  def get_logprob(agent, outputs):     '''     Get the logprobs for outputs acc. to agent's policy.      Args:         agent: Actor network (or reference)         outputs: output ids             Shape = (sequence, tokens)      returns         logprob of outputs acc to agent's policy             Shape = (sequence, tokens)     '''     logits = agent(outputs[:, :-1])[0][:, -INPUT_SIZE:, :]     logprob = Categorical(logits=logits).log_prob(outputs[:, -INPUT_SIZE:])     return logprob <p>Hyperparameters --</p> In\u00a0[\u00a0]: Copied! <pre># Hyperparams\nepochs = 100\nactor_lr = 1e-5\ncritic_lr = 1e-4\ntrain_actor_iter = 4 # Train the networks this many times per epoch\ntrain_critic_iter = 4\nclip_ratio = 0.2 # PPO Clip\ngamma = 0.99 # Discount factor\nkl_beta = 1 # KL coeff for reward\nsave = False\n\n# For plotting\nrew_list = []\nkl_list = []\n</pre> # Hyperparams epochs = 100 actor_lr = 1e-5 critic_lr = 1e-4 train_actor_iter = 4 # Train the networks this many times per epoch train_critic_iter = 4 clip_ratio = 0.2 # PPO Clip gamma = 0.99 # Discount factor kl_beta = 1 # KL coeff for reward save = False  # For plotting rew_list = [] kl_list = [] <p>Here we set up our models and optimizers. We typically need 3 models for RLHF training:</p> <ol> <li>Actor: This is the LLM that we will fine-tune using reinforcement learning(RL). It is initialised as a copy of the pretrained model.</li> <li>Reference: To prevent the actor's output distribution (or \"policy\" in RL terms) from diverging too much from the pretrained model's distribution, we need to apply some constraint on the distance/difference of the two distributions. For this, we keep this reference model which is a frozen copy of the pretrained model to calculate KL divergence during our RL training.</li> <li>Critic: The critic network is also a copy of the base LLM but with the last layer replaced with a single output. This is used to estimate the value function, which is a component required to calculate the actor's loss.</li> </ol> <p>In our simple problem statement, the rewards are given at the end of the sequence. Therefore, we don't need to estimate the value function and hence, don't train a critic network. For more information, see this answer on StackExchange.</p> In\u00a0[\u00a0]: Copied! <pre># Actor\nactor = get_model(block_size=INPUT_SIZE*2, vocab_size=st_dataset.get_vocab_size())\nactor.load_state_dict(torch.load(\"models/minimal_RLHF_basic_supervised.pt\")) # Load ST model from disk\n# Reference\nreference = get_model(block_size=INPUT_SIZE*2, vocab_size=st_dataset.get_vocab_size())\nreference.load_state_dict(torch.load(\"models/minimal_RLHF_basic_supervised.pt\")) # Clone of actor\n\n# Optimizers\nactor_opt = torch.optim.AdamW(actor.parameters(), lr=actor_lr)\n\n# Set models to train/eval\nreference.eval()\nreward_model.eval()\nactor.train()\n</pre> # Actor actor = get_model(block_size=INPUT_SIZE*2, vocab_size=st_dataset.get_vocab_size()) actor.load_state_dict(torch.load(\"models/minimal_RLHF_basic_supervised.pt\")) # Load ST model from disk # Reference reference = get_model(block_size=INPUT_SIZE*2, vocab_size=st_dataset.get_vocab_size()) reference.load_state_dict(torch.load(\"models/minimal_RLHF_basic_supervised.pt\")) # Clone of actor  # Optimizers actor_opt = torch.optim.AdamW(actor.parameters(), lr=actor_lr)  # Set models to train/eval reference.eval() reward_model.eval() actor.train() <p>At last, we come to our main RL training. We use PPO, a famous RL algorithm for fine-tuning our model along with a KL divergence penalty.</p> <p>PPO: The main idea behind PPO is to induce stability in the training process by preventing large updates.</p> <p>Let's look at the PPO loss:</p> <p>$$L = \\text{min}\\biggl( \\frac{\\pi_{k+1} (a|s)}{\\pi_{k} (a|s)} R, \\text{  clip}\\Bigl(\\frac{\\pi_{k+1} (a|s)}{\\pi_{k} (a|s)}, 1-\\epsilon, 1+\\epsilon\\Bigr) R\\biggr)$$</p> <p>Where $\\pi_k$ represents the policy at $k$'th training step, R is reward and $\\epsilon$ is a hyperparameter for clipping the policy update. I have partly modified the loss to prevent too many new ideas at once for beginners. This version is sufficient for our current case. To learn more about the PPO loss, look at SpinningUp and Eric's article.</p> <p>The PPO loss looks complicated but is fairly straightforward. To understand what the PPO loss does, consider the two cases:</p> <ol> <li>R is positive</li> <li>R is negative</li> </ol> <p>Case 1: R is positive. Then the loss reduces to $$L = \\text{min}\\biggl( \\frac{\\pi_{k+1} (a|s)}{\\pi_{k} (a|s)}, 1+\\epsilon \\biggr)R$$ So if the policy at the next training step is increasing too far from the previous step, we clip it to $1+\\epsilon$.</p> <p>Case 2: R is negative. Then the loss reduces to $$L = \\text{max}\\biggl( \\frac{\\pi_{k+1} (a|s)}{\\pi_{k} (a|s)}, 1-\\epsilon \\biggr)|R|$$ So if the policy at the next training step is decreasing too far from the previous step, we clip it to $1-\\epsilon$.</p> <p>KL Divergence: KL divergence is a measure of difference between two distributions. We use KL divergence penalty to ensure our actor's policy (probability distribution over next tokens) does not stray too far from the reference model's policy. For two distributions P and Q defined on the same sample space, $X$, the KL divergence is given by:</p> <p>$$D_{KL}(P||Q) = \\sum_{x \\in X} P(x) log\\biggl(\\frac{P(x)}{Q(x)}\\biggr)$$</p> <p>The final reward used for PPO is then a linear combination of the scalar output from the reward model $R_{RM}$ and the value of KL divergence $R_{KL}$ with a hyperparameter $\\beta$.</p> <p>$$R = R_{RM} - \\beta R_{KL}$$</p> <p>Both of PPO and KL divergence are crucial components to the RLHF training due to the inherent fragile nature of RLHF.  Mainly, the issue lies with the reward model and the fact that it cannot completely capture human preferences. The data used to train the reward model is generated using the base LLM's policy. Therefore, if the actor diverges too far from the base policy and the reward model is asked to give feedback for samples that do not come from the training distribution, we cannot predict the behaviour of the reward model. In fact, this exact issue often leads to adversarial training (see Deepak's article on Reward Hacking in LLMs). This issue is avoided by taking small steps in PPO and using a KL penalty to prevent moving too far from base policy.</p> <p>Now we have the RL code --</p> In\u00a0[\u00a0]: Copied! <pre># Dataloader - we use the same as reward model for now since we only need the inputs and it's in the correct format for what we want in RLHF training\n# Can't use the supervised training's dataloader directly since the input has some part of output concatenated in that\nft_dataloader = train_loader\n\n# Train\nfor ep in range(epochs):\n    for b_i, batch in enumerate(ft_dataloader):\n        # Get some inputs from supervised dataset (only inputs - we don't care about the ground truths anymore)\n        inp, _, __ = batch\n\n        # Generate output sequence\n        out = actor.generate(inp, max_new_tokens=INPUT_SIZE, do_sample=True) # Not sampling since good and bad in our problem is fairly deterministic, otherwise prefer to sample.\n        start_logprob = get_logprob(actor, out).detach()\n        start_logprob = start_logprob.sum(-1)\n\n        # Reward\n        rew_out = torch.concat([out, torch.Tensor([[11]]*out.shape[0])], dim=-1).long() # Add [CLS] = 11\n        rew = reward_model(rew_out)[0][:, -1, 0]\n        rew_list.append(rew.mean().item())\n\n        # Actor train loop\n        for _iter_actor in range(train_actor_iter):\n            # Get logprobs\n            cur_logprob = get_logprob(actor, out)\n            ref_logprob = get_logprob(reference, out)\n            cur_logprob = cur_logprob.sum(dim=-1) # Summing because we don't have rewards for each timestep\n            ref_logprob = ref_logprob.sum(dim=-1)\n\n            # KL and reward update\n            kl_div = (cur_logprob - ref_logprob).detach()\n            rew = rew - kl_beta * kl_div\n\n            # PPO loss\n            ratio = torch.exp(cur_logprob - start_logprob)\n            clip_rat = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio)\n            actor_loss = -(torch.min(ratio * rew, clip_rat * rew)).mean()\n\n            # Update actor\n            actor_opt.zero_grad()\n            actor_loss.backward(retain_graph=True)\n            actor_opt.step()\n\n        # Save kl div for plotting\n        kl_list.append(kl_div.mean().item())\n\n        # Eval\n        if ep % 1 == 0 and b_i % 50 == 0:\n            print(f\"Epoch={ep} -- batch={b_i} || \" + \\\n                    f\"Reward={round(rew_list[-1], 2)} || \" + \\\n                    f\"KLD={round(kl_list[-1], 2)} || \" + \\\n                    f\"actor loss={round(actor_loss.item(), 2)}\")\n            print(out[0])\n            print(\"#\"*100)\n</pre> # Dataloader - we use the same as reward model for now since we only need the inputs and it's in the correct format for what we want in RLHF training # Can't use the supervised training's dataloader directly since the input has some part of output concatenated in that ft_dataloader = train_loader  # Train for ep in range(epochs):     for b_i, batch in enumerate(ft_dataloader):         # Get some inputs from supervised dataset (only inputs - we don't care about the ground truths anymore)         inp, _, __ = batch          # Generate output sequence         out = actor.generate(inp, max_new_tokens=INPUT_SIZE, do_sample=True) # Not sampling since good and bad in our problem is fairly deterministic, otherwise prefer to sample.         start_logprob = get_logprob(actor, out).detach()         start_logprob = start_logprob.sum(-1)          # Reward         rew_out = torch.concat([out, torch.Tensor([[11]]*out.shape[0])], dim=-1).long() # Add [CLS] = 11         rew = reward_model(rew_out)[0][:, -1, 0]         rew_list.append(rew.mean().item())          # Actor train loop         for _iter_actor in range(train_actor_iter):             # Get logprobs             cur_logprob = get_logprob(actor, out)             ref_logprob = get_logprob(reference, out)             cur_logprob = cur_logprob.sum(dim=-1) # Summing because we don't have rewards for each timestep             ref_logprob = ref_logprob.sum(dim=-1)              # KL and reward update             kl_div = (cur_logprob - ref_logprob).detach()             rew = rew - kl_beta * kl_div              # PPO loss             ratio = torch.exp(cur_logprob - start_logprob)             clip_rat = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio)             actor_loss = -(torch.min(ratio * rew, clip_rat * rew)).mean()              # Update actor             actor_opt.zero_grad()             actor_loss.backward(retain_graph=True)             actor_opt.step()          # Save kl div for plotting         kl_list.append(kl_div.mean().item())          # Eval         if ep % 1 == 0 and b_i % 50 == 0:             print(f\"Epoch={ep} -- batch={b_i} || \" + \\                     f\"Reward={round(rew_list[-1], 2)} || \" + \\                     f\"KLD={round(kl_list[-1], 2)} || \" + \\                     f\"actor loss={round(actor_loss.item(), 2)}\")             print(out[0])             print(\"#\"*100) <p>Output is ommitted for brevity. Here is an image of some of the outputs:</p> <p></p> <p>Save model --</p> In\u00a0[\u00a0]: Copied! <pre>import datetime, os, json\n\nsave = True\n\n# RUN TO SAVE MODEL\nfolder = f\"models/08_min_rlhf_basic_{datetime.datetime.now().__str__()}\"\nos.makedirs(folder, exist_ok=True)\n\ntorch.save(reward_model, f\"{folder}/reward_nodel.pt\")\ntorch.save(reference, f\"{folder}/reference.pt\")\ntorch.save(actor, f\"{folder}/actor.pt\")\n\nwith open(f\"{folder}/config.json\", 'w') as f:\n    json.dump({\n        \"epochs\": epochs,\n        \"actor_lr\": actor_lr,\n        \"critic_lr\": critic_lr,\n        \"train_actor_iter\": train_actor_iter,\n        \"train_critic_iter\": train_critic_iter,\n        \"clip_ratio\": clip_ratio,\n        \"gamma\": gamma,\n        \"kl_beta\": kl_beta,\n    }, f)\n</pre> import datetime, os, json  save = True  # RUN TO SAVE MODEL folder = f\"models/08_min_rlhf_basic_{datetime.datetime.now().__str__()}\" os.makedirs(folder, exist_ok=True)  torch.save(reward_model, f\"{folder}/reward_nodel.pt\") torch.save(reference, f\"{folder}/reference.pt\") torch.save(actor, f\"{folder}/actor.pt\")  with open(f\"{folder}/config.json\", 'w') as f:     json.dump({         \"epochs\": epochs,         \"actor_lr\": actor_lr,         \"critic_lr\": critic_lr,         \"train_actor_iter\": train_actor_iter,         \"train_critic_iter\": train_critic_iter,         \"clip_ratio\": clip_ratio,         \"gamma\": gamma,         \"kl_beta\": kl_beta,     }, f) <p>Plot rewards --</p> In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\nimport numpy as np\n\ndef moving_average(a, n=10):\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\nplt.plot(moving_average(rew_list, 20), label=\"reward\")\nplt.plot(moving_average(kl_list, 20), label=\"kl\")\nplt.legend()\nplt.title(f\"Reward plot || KL beta = {kl_beta}\")\nplt.xlabel(\"Train steps\")\nplt.ylabel(\"Reward (moving average)\")\nif save:\n    plt.savefig(f\"{folder}/plot.png\")\nelse:\n    plt.show()\n</pre> from matplotlib import pyplot as plt import numpy as np  def moving_average(a, n=10):     ret = np.cumsum(a, dtype=float)     ret[n:] = ret[n:] - ret[:-n]     return ret[n - 1:] / n  plt.plot(moving_average(rew_list, 20), label=\"reward\") plt.plot(moving_average(kl_list, 20), label=\"kl\") plt.legend() plt.title(f\"Reward plot || KL beta = {kl_beta}\") plt.xlabel(\"Train steps\") plt.ylabel(\"Reward (moving average)\") if save:     plt.savefig(f\"{folder}/plot.png\") else:     plt.show() <p>With our hyperparameters, we have maximized reward while maintaining a non-divergent KL. Looking at the outputs above, the model seems to behave the way we want it to.</p> <p>Hurray!</p> <p>First I run some visualizations on the base LLM to set the stage.</p> <p>Let's look at some probability plots over the output tokens for different input lengths.</p> In\u00a0[\u00a0]: Copied! <pre>x, y = torch.Tensor([[5, 7, 1, 5, 10]]).long(), torch.Tensor([[6, 2, 7, 3]]).long()\n</pre> x, y = torch.Tensor([[5, 7, 1, 5, 10]]).long(), torch.Tensor([[6, 2, 7, 3]]).long() In\u00a0[\u00a0]: Copied! <pre>fig, axes = plt.subplots(1, 4, sharey=True, figsize=(15, 3))\nfor i in range(4):\n    _x = torch.concat([x, y[:, :i]], dim=1)\n    axes[i].bar(np.arange(11), torch.softmax((reference(_x)[0][0, -1].detach()), dim=0).tolist())\n    axes[i].title.set_text(f\"Input {_x[0].tolist()}\")\n    axes[i].set_ylim([0, 1])\naxes[0].set_ylabel(\"Probability\")\n</pre> fig, axes = plt.subplots(1, 4, sharey=True, figsize=(15, 3)) for i in range(4):     _x = torch.concat([x, y[:, :i]], dim=1)     axes[i].bar(np.arange(11), torch.softmax((reference(_x)[0][0, -1].detach()), dim=0).tolist())     axes[i].title.set_text(f\"Input {_x[0].tolist()}\")     axes[i].set_ylim([0, 1]) axes[0].set_ylabel(\"Probability\") Out[\u00a0]: <pre>Text(0, 0.5, 'Probability')</pre> <p>The supervised model learns an almost equal probability over the increment of last 4 tokens. This was our intended behaviour.</p> <p>The exact probabilities vary but are mostly within a some threshold of each other. For the first plot though, since the input contains two 5's, the probability of 6 is much higher than others.</p> <p>Now, let's look at what the attention heads focus on. I'm using BertViz for visualizing attention weights.</p> <p></p> <p>Here also, the model successfully learns to look at the last 4 input tokens while ignoring the separation token.</p> <p>We look at the same visualizations for the fine tuned model with $\\beta = 1$.</p> In\u00a0[\u00a0]: Copied! <pre>fig, axes = plt.subplots(1, 4, sharey=True, figsize=(15, 3))\nfor i in range(4):\n    _x = torch.concat([x, y[:, :i]], dim=1)\n    axes[i].bar(np.arange(11), torch.softmax((actor(_x)[0][0, -1].detach()), dim=0).tolist())\n    axes[i].title.set_text(f\"Input {_x[0].tolist()}\")\n    axes[i].set_ylim([0, 1])\naxes[0].set_ylabel(\"Probability\")\n</pre> fig, axes = plt.subplots(1, 4, sharey=True, figsize=(15, 3)) for i in range(4):     _x = torch.concat([x, y[:, :i]], dim=1)     axes[i].bar(np.arange(11), torch.softmax((actor(_x)[0][0, -1].detach()), dim=0).tolist())     axes[i].title.set_text(f\"Input {_x[0].tolist()}\")     axes[i].set_ylim([0, 1]) axes[0].set_ylabel(\"Probability\") Out[\u00a0]: <pre>Text(0, 0.5, 'Probability')</pre> <p></p> <p>The model learnt to change the distribution over the first output token only. For the first plot and attention figure, the model learns to focus on a single token and output its increment. As for the rest of the tokens, it retains a similar behavior as the base model.</p> <p>As as additional exercise, we also look at how the model behaves when no KL divergence penalty is applied.</p> <p>I've run the training separately and compiled the results here.</p> <p></p> <p>The reward hits the max but the KL is diverging, which means that the policy we are learning is moving further away from the base distribution as the training goes on. This is a result of not applying a penalty to KL divergence.</p> In\u00a0[\u00a0]: Copied! <pre>fig, axes = plt.subplots(1, 4, sharey=True, figsize=(15, 3))\nkl0_actor = torch.load(\"models/kl_beta=0/actor.pt\")\nfor i in range(4):\n    _x = torch.concat([x, y[:, :i]], dim=1)\n    axes[i].bar(np.arange(11), torch.softmax((kl0_actor(_x)[0][0, -1].detach()), dim=0).tolist())\n    axes[i].title.set_text(f\"Input {_x[0].tolist()}\")\n    axes[i].set_ylim([0, 1])\naxes[0].set_ylabel(\"Probability\")\n</pre> fig, axes = plt.subplots(1, 4, sharey=True, figsize=(15, 3)) kl0_actor = torch.load(\"models/kl_beta=0/actor.pt\") for i in range(4):     _x = torch.concat([x, y[:, :i]], dim=1)     axes[i].bar(np.arange(11), torch.softmax((kl0_actor(_x)[0][0, -1].detach()), dim=0).tolist())     axes[i].title.set_text(f\"Input {_x[0].tolist()}\")     axes[i].set_ylim([0, 1]) axes[0].set_ylabel(\"Probability\") Out[\u00a0]: <pre>Text(0, 0.5, 'Probability')</pre> <p>The first output is correct but the rest are all messed up. Let's look at the attention heads.</p> <p></p> <p>The model seems to have learnt to not look at tokens before the last input token. We can see this in the output probability plots too, that the tokens before 5 do not have high probability. That is only for the input though. The effect of sampling from the output on the probability distribution is harder to interpret since we do not have a way to visualize how the weights affect the probability during the forward pass.</p>"},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#rlhf-an-independent-illustration","title":"RLHF - An Independent Illustration\u00b6","text":"<p>Author: Abhor Gupta | Published on: 14 March, 2024 (\ud83d\ude0a \u03c0 day)</p>"},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#overview","title":"Overview\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#training-llms","title":"Training LLMs\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#rlhf-components","title":"RLHF components\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#applications-of-rlhf","title":"Applications of RLHF\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#problem-statement","title":"Problem Statement\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#language-supervised-learning","title":"Language (supervised learning)\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#alignment-rlhf","title":"Alignment (RLHF)\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#code-and-commentary","title":"Code and Commentary\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#supervised-pre-training","title":"Supervised pre-training\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#training-a-reward-model","title":"Training a reward model\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#rl-fine-tuning","title":"RL fine-tuning\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#explainability-interpretability","title":"Explainability / Interpretability\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#base-models-visualizations","title":"Base model's visualizations\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#fine-tuned-model-with-beta-1","title":"Fine tuned model with beta = 1\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#fine-tuned-model-with-beta-0","title":"Fine tuned model with beta 0\u00b6","text":""},{"location":"session_4/part_2_finetuning_lms_to_human_preferences/RLHF/#conclusion","title":"Conclusion\u00b6","text":"<p>In conclusion, Reinforcement Learning with Human Feedback (RLHF) is a promising approach to incorporate human expertise into deep learning models. Through our minimal example, we've made an attempt at explaining how RLHF integrates human feedback into the reinforcement learning process, and how it tries to align the learning of the model towards human preferences. \ud83c\udf89</p>"}]}